---
layout: single  
title: "[Paper Review] Graph Attention Networks"  
date: 2025-05-23  
permalink: /gat/  
categories:
    - Graph Theory
    - Machine Learning    
    - Graph Signal Processing  
tags:  
    - Graph
    - Graph Attention
    - GAT  

toc: true  
toc_sticky: true  
---

> 이 포스팅은 Velickovic et al. (2018) 의 논문 [Graph Attention Networks](https://arxiv.org/abs/1710.10903)를 읽고 정리한 글입니다.

## Graph Attention Mechanisms

그래프에서 각 노드가 이웃으로부터 얻는 정보의 중요도는 상황에 따라 달라질 수 있습니다.
**그래프 어텐션 네트워크(GAT)** 는 이러한 차이를 학습 가능한 가중치로 표현하여
노드 임베딩을 생성합니다.

- Self-attention을 활용해 **이웃별로 다른 가중치**를 두고 특징을 모읍니다.
- 노드 특성의 선형 변환 후, 인접 노드 간의 “유사도”를 계산하여 가중치를 정규화합니다.

---

## Model Architecture

각 GAT 레이어는 다음과 같은 순서로 동작합니다.

1. **선형 변환**: 노드 $i$의 입력 특징 $h_i \in \mathbb{R}^F$에 대해 학습 파라미터 $W \in \mathbb{R}^{F'\times F}$ 를 곱해 $Wh_i$ 를 만듭니다.
2. **Attention Coefficient**: 이웃 노드 $j\in\mathcal{N}(i)$에 대해
   \[
   e_{ij} = a\bigl(Wh_i,\, Wh_j\bigr),
   \]
   여기서 $a(\cdot,\cdot)$는 작은 신경망(보통 단일 선형 계층 뒤 LeakyReLU)입니다.
3. **정규화**: 소프트맥스를 사용해 각 이웃의 중요도를 얻습니다.
   \[
   \alpha_{ij} = \mathrm{softmax}_j\bigl(e_{ij}\bigr)
   = \frac{\exp(e_{ij})}{\sum_{k\in\mathcal{N}(i)} \exp(e_{ik})}.
   \]
4. **특징 집계**: 정규화된 가중치로 이웃 특징을 합산하고 비선형 함수를 적용합니다.
   \[
   h_i' = \sigma\Bigl(\sum_{j\in\mathcal{N}(i)} \alpha_{ij} \, W h_j\Bigr).
   \]
5. **멀티헤드 어텐션**: 여러 개의 어텐션을 병렬로 수행한 뒤 평균 또는 연결(concat)합니다.

이 과정을 여러 층 쌓아 노드 표현을 깊게 학습합니다.

### Pseudo-code

```python
for l in range(num_layers):
    H = W[l] @ H                  # linear transform
    e = LeakyReLU(a[l](H_i, H_j)) # pair-wise scores
    alpha = softmax(e, dim=neighbors)
    H = sigma(sum(alpha * H_j))   # weighted aggregation
```

---

## Implementation Notes

- 인접 행렬을 **희소 텐서**로 처리하여 메모리를 절약합니다.
- 멀티헤드 어텐션의 출력을 평균하여 안정적인 학습을 도모합니다.
- 작은 데이터셋(Cora 등)에서는 드롭아웃과 L2 정규화를 함께 사용합니다.

---

## Applications

- 노드 분류, 링크 예측, 그래프 수준 분류 등 **다양한 그래프 작업**에 사용됩니다.
- 이웃 간 중요도를 가중치로 학습하므로 **구조가 불규칙하거나 동적인 그래프**에도 유연합니다.
- 어텐션 가중치를 시각화하면 모델의 **설명 가능성**을 어느 정도 확보할 수 있습니다.

