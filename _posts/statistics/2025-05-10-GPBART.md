---
layout: single
title: "[Paper Review] GP-BART: a novel Bayesian additive regression trees approach using Gaussian processes"
date: 2025-05-10
permalink: /gp-bart/
categories:
  - Statistics
tags: 
  - Statistics
  - Bayesian
  - GP-BART
  - BART
  - Machine Learning
  - Bayesian Additive Regression Tree
  - Regression Tree
  - Classification
toc: true
toc_sticky: true
---

>이 포스팅은 Maia et al. (2022)의 논문 ["GP-BART: a novel Bayesian additive regression trees approach using Gaussian processes"](https://arxiv.org/pdf/2204.02112)를 읽고 정리한 글입니다.

---

## Introduction

>BART(Bayesian Additive Regression Tree)에 대한 설명은 [BART](/bart/) 글을 참고하세요.

### Motivation
기존의 BART는 각 Leaf Node에 상수값($\mu_i$)를 부여함으로써 예측 함수를 구성합니다. 
이 방식은 단순하고 계산의 효율성이 높지만, 예측 함수가 **불연속**하게 되어 다음과 같은 한계가 있습니다.

- 입력 변수 공간에서의 Smooth한 함수의 추정이 어려움.
- 공간적, 연속적 상관관계는 모델링하지 못함
- 리프 노드 간의 예측값이 급격히 변하는 현상이 발생함

이러한 점에서 BART는 Spatial Regression, Time Series Analysis 등에서 부적절할 수 있습니다. 

---

## Idea 

> Leaf Node에 상수값이 아닌 **Gaussian Process(GP)** 를 도입하여, 각 영역에서 **부드러운** 함수 형태로 예측을 수행하도록 확장해보자!


GP에 대한 글은 [Gaussian Process](/gp/) 글을 참고해주세요

---

## Model

GP-BART는 기존 BART의 **sum-of-trees framework**를 유지하면서,  
각 Tree의 **Leaf Node 출력값**을 상수 $\mu$가 아닌, **Gaussian Process(GP)**로 대체한 구조입니다.

### A mathematical expression

$$
f(x) = \sum_{j=1}^m g_j(x), \quad g_j(x) = f^{(j)}(x; T_j, \text{GP}_j)
$$

- $T_j$: $j$번째 트리 구조 (분할 규칙)
- $\text{GP}_j$: $T_j$의 각 말단 노드에 정의된 GP

### Difference between BART and GP-BART

| 항목 | BART | GP-BART |
|------|------|---------|
| Leaf node 값 | 상수 $\mu$ | GP |
| 예측 함수 | 계단형 (piecewise constant) | 부드러운 곡선 (smooth) |
| 연속성 | 없음 | 존재 |
| 표현력 | 제한적 | 향상됨 |

즉, GP-BART는 각 트리가 "작은 지역 함수 조각(local GP smoother)"를 표현하도록 하며,  
전체적으로는 **부드럽고 유연한 함수의 합**으로 target function $f(x)$를 추정합니다.

---

## Inference

기본적으로는 BART에서 사용하던 **Bayesian Backfitting MCMC** 방식을 확장합니다.

### Posterior Sampling Procedure

1. 트리 구조 $T_j$를 grow/prune/change/swap 방식으로 샘플링
2. 각 리프 노드에 속한 **GP의 하이퍼파라미터 및 함수값**을 샘플링
3. 전체 예측 함수 $f(x)$를 업데이트
4. 오차 분산 $\sigma^2$ 샘플링

### Main Changes

- 리프 노드마다 **독립적인 GP**가 존재하며,
- 각 노드는 자신에게 속한 데이터 포인트만으로 **local GP regression**을 수행
- 따라서 각 트리를 업데이트할 때마다 해당 영역의 GP posterior도 함께 업데이트

> 결과적으로, GP-BART는 "sum of local GP models"로 볼 수 있으며,  
> 기존 BART보다 계산량은 늘어나지만 훨씬 부드럽고 정밀한 예측이 가능합니다.

---

## Results

논문에서는 다양한 synthetic data 및 real-world regression task에서 GP-BART의 성능을 평가합니다.

- **RMSE 기준으로** 기존 BART 대비 일관된 성능 향상
- 특히 연속성이 중요한 domain (e.g. spatial, functional data)에서 강력함
- 예측 곡선이 매우 부드럽고, credible interval이 자연스러움

> 예측의 부드러움과 불확실성 추정 측면에서 GP-BART는 Random Forest, BART, Gaussian Process 단독 모델보다 우수한 결과를 보임

---

## Discussion

### Advantages

- **예측의 연속성 확보**: 불연속적이고 경직된 예측을 방지
- **국소 적응성(Local adaptivity)**: 각 리프마다 GP를 갖기 때문에 지역적 패턴에 민감하게 반응
- **베이지안적 불확실성 추정**: 예측과 함께 신뢰 구간도 제공

### Disadvantages

- **계산 비용 증가**: 모든 리프에서 GP inference를 수행해야 하므로 MCMC 비용 증가
- **하이퍼파라미터 설정**: GP의 커널 파라미터를 트리마다 추정해야 하므로 tuning이 중요함

---

## Conclusion

GP-BART는 기존 BART의 한계를 Gaussian Process를 통해 극복한 강력한 베이지안 회귀 모델입니다.  
Tree-based 모델의 해석 가능성과, GP의 연속성 및 불확실성 추정 능력을 통합하여,  
다양한 실제 문제에서 더욱 유연하고 강건한 추정을 가능하게 합니다.

---

## Reference

- [GP-BART: a novel Bayesian additive regression trees approach using Gaussian processes](https://arxiv.org/pdf/2204.02112)
- [BART: Bayesian Additive Regression Trees](https://arxiv.org/pdf/0806.3286)
- [Gaussian Process(GP) 설명 포스트](/gp/)