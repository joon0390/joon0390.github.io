---
layout: single  
title: "[Paper Review] Gaussian Processes Over Graphs"  
date: 2025-05-23  
permalink: /GPG/  
categories:  
  - Machine Learning  
  - Gaussian Processes  
  - Graph Signal Processing  
tags:  
  - Gaussian Process  
  - Graphs  
  - Bayesian  
  - GPG  

toc: true  
toc_sticky: true  
---

> 이 포스팅은 Venkitaraman et al. (2018) 의 논문 [Gaussian Processes Over Graphs](https://arxiv.org/pdf/1803.05776)를 읽고 정리한 글입니다.

---

## Introduction

> 그래프에 대한 개념은 [Graph](/graph/) 글을 참고하세요.

<br/>

전통적인 Gaussian Process(GP)는 **입력** 간의 유사도만을 반영해 함수값을 모델링하지만, **출력 벡터**의 구조(예: 그래프 연결)에는 무관심합니다. 본 논문은 **그래프 라플라시안**을 GP prior에 결합함으로써, 노드 간 ‘매끄러움(smoothness)’ 제약을 추가한 **GPG (Gaussian Processes on Graphs)** 를 제안합니다.  

- **문제**: GP의 predictive variance가 소량·노이즈 데이터에서 과도하게 큼.  
- **해결**: 출력 노드 차원에서 $(I + \alpha L)^{-1}$ 스무딩 필터를 공분산에 삽입.  
- **주장**: GPG는 non‐trivial 그래프에서 항상 predictive variance를 줄이며, 실제 데이터에서도 성능 우수.

---

## Background: Graph Signal Processing

### Graph Laplacian  
무향 가중치 그래프 $G=(V,E)$에 대해  
$$
L = D - A,\quad D_{ii}=\sum_j A_{ij}
$$  
이며,  
$$
y^\top L\,y = \sum_{(i,j)\in E}A_{ij}(y_i-y_j)^2
$$  
로 노드 값 차이의 제곱합(매끄러움)의 척도가 됩니다.

<br/>

### Graph Fourier & Smoothing Filter  
라플라시안 고유분해 $L=V\Lambda V^\top$를 통해 주파수 성분을 분해하면,  
$$
(I + \alpha L)^{-1} = V\,(I + \alpha\Lambda)^{-1}\,V^\top
$$  
로 **저역(low-pass) 필터**로 해석할 수 있습니다.

---

## Methodology

### 1. Graph-Smoothing Prior  
출력 벡터 $\mathbf{f}\in\mathbb R^M$에 대해  
$$
p(\mathbf{f}) 
\propto
\exp\Bigl(-\tfrac12\,\mathbf{f}^\top (I + \alpha L)\,\mathbf{f}\Bigr)
\quad\Longrightarrow\quad
\mathbf{f}\sim\mathcal N\bigl(0,\,(I+\alpha L)^{-1}\bigr).
$$

<br/>

### 2. GPG Covariance Function  
Bayesian 선형 회귀 관점의 전통 GP:  
$$
K_{\rm GP}(x,x')
=\Phi(x)\,\tfrac1\alpha I\,\Phi(x')^\top.
$$  
GPG에서는 여기에 그래프 스무딩을 추가하여  
$$
\boxed{
K_{\rm GPG}(x,x')
= \Phi(x)\,(I+\alpha L)^{-1}\,\Phi(x')^\top
}
$$  
을 사용합니다.  
- Toy 예제: $\Phi(x)=xI$ 로 두면 $K_{\rm GPG}(x,x')=x\,x'\,(I+\alpha L)^{-1}$.

<br/>

### 3. Variance Reduction Proof  
GPG가 기존 GP보다 예측 분산을 줄이는 이유는 다음과 같습니다:

1. **PSD 순서 관계**  
   - $L\succeq0$ 이므로 $I+\alpha L\succeq I$  
   - 양의 정부호 행렬의 역행렬은 순서를 반전: $(I+\alpha L)^{-1}\preceq I$  

2. **Prior 공분산 비교**  
   $$
     K_{\rm GPG}(x,x')
     = \Phi(x)\,(I+\alpha L)^{-1}\,\Phi(x')^\top
     \preceq
     \Phi(x)\,I\,\Phi(x')^\top
     = \alpha\,K_{\rm GP}(x,x').
   $$

3. **Predictive Covariance 부등식**  
   GP의 predictive covariance는
   $\Sigma_* = k_{**} - k_*^\top\,(K + \beta^{-1}I)^{-1}k_*$ 입니다.  
   Prior 공분산 $K_{\rm GPG}\preceq K_{\rm GP}$ 이므로  
   $$
     (K_{\rm GPG}+\beta^{-1}I)^{-1} \succeq (K_{\rm GP}+\beta^{-1}I)^{-1},
     \quad
     k_*^{\rm GPG}\preceq k_*^{\rm GP},
     \quad
     k_{**}^{\rm GPG}\preceq k_{**}^{\rm GP}.
   $$  
   이를 종합하면
   $$
     \Sigma_*^{\rm GPG} \;\preceq\; \Sigma_*^{\rm GP},
   $$
   즉 **예측 분산이 항상 감소**함을 보장합니다.

<br/>

### 4. Inference & Predictive Distribution  
학습세트 $\{x_i,\mathbf{t}_i\}_{i=1}^N$에 대해  
$$
\begin{pmatrix} T \\ \mathbf{f}_* \end{pmatrix}
\sim
\mathcal N\!\Bigl(0,\,
\begin{pmatrix}
K + \beta^{-1}I & k_*\\
k_*^\top & k_{**}
\end{pmatrix}\Bigr),
$$

$$
\boxed{
\mathbf{f}_* \mid T
\sim
\mathcal N\bigl(\mu_*,\,\Sigma_*\bigr),
\quad
\mu_*=k_*^\top(C)^{-1}T,\;
\Sigma_*=k_{**}-k_*^\top(C)^{-1}k_*,
}
$$  
where $C=K+\beta^{-1}I$.

---

## Experiments (Section IV)

### A. Synthetic Chain-Graph Signal  
1. **Graph**: 5-node chain; compute $L$ and $F=(I+\alpha L)^{-1}$.  
2. **Signal**: smooth + noise; mask nodes 2,3; train on {1,4,5}.  
3. **Kernel**: RBF on node index + GPG filter.  
4. **Results**: GPG predictions closer to true vs. vanilla GP.  
5. **Visualization**: 그래프 예측 결과를 플롯하여 비교.

### B. Real-World Graph Signals  
- **Weather Stations**: 지리적 인접성 그래프 → 온도 예측  
- **fMRI**: 뇌 영역 간 functional 연결 → 뇌 신호 복원  
- **Results**: GPG가 RMSE↓, predictive interval coverage↑

### C. Hyperparameter Sensitivity  
- $\alpha$ (smoothness strength) 및 $\beta$ (noise) 변화 실험  
- 적절한 $\alpha$ 범위 제시

```python
import numpy as np, matplotlib.pyplot as plt
from scipy.linalg import inv

# L, F 계산… (생략)
# y 생성, K_gp, K_gpg 구성
# train/test 분리, 예측 수행
# 시각화
plt.plot(test_idx, y[test_idx], 'o-')
plt.plot(test_idx, pred_gp, 's--')
plt.plot(test_idx, pred_gpg, 'd-.')
plt.legend(['True','GP','GPG'])
```

---

## Conclusion

“Gaussian Processes Over Graphs”는 **그래프 라플라시안**을 GP prior에 결합해 출력 노드 간 smoothness를 강제함으로써,  
- 예측 분산을 수학적으로 **항상** 줄이고,  
- 실험적으로 소량·노이즈 환경에서 RMSE 및 신뢰구간 성능을 크게 개선합니다.  


---

## References

- [Giraldo, L. M., Zhang, H., & Ribeiro, A. (2020). *Gaussian Processes Over Graphs*. IEEE Transactions on Signal Processing.](https://arxiv.org/pdf/1803.05776)
- [Rasmussen, C. E., & Williams, C. K. I. (2006). *Gaussian Processes for Machine Learning*. MIT Press.](https://gaussianprocess.org/gpml/chapters/RW.pdf)  

