---
layout: single  
title: "[Paper Review] Graph Attention Networks"  
date: 2025-05-23  
permalink: /gat/  
categories:
    - Graph Theory
    - Machine Learning    
    - Graph Signal Processing  
tags:  
    - Graph
    - Graph Attention
    - GAT  

toc: true  
toc_sticky: true  
---

> 이 포스팅은 Velickovic et al. (2018) 의 논문 [Graph Attention Networks](https://arxiv.org/abs/1710.10903)를 읽고 정리한 글입니다.


*그래프에 대한 개념은 다음의 글들을 다음 글들에 정리되어 있습니다.

- [Graph](/graph/ )
- [Graph Convolutional Networks](/gcn/) 

---

## Introduction

### Motivation

기존의 그래프 신경망(Graph Neural Networks, GNN)들은 주로 **고정된 가중치 기반의 이웃 평균화** 방식을 사용했습니다. 대표적으로 [GCN (Kipf & Welling, 2017)](https://arxiv.org/abs/1609.02907)은 노드의 표현을 주변 이웃의 feature로부터 평균적으로 aggregate하는 방식으로 설계되었습니다. 그러나 이러한 방식은 다음과 같은 한계를 가집니다:

- 이웃 노드의 중요도를 구분할 수 없음 (모든 이웃이 동일한 비중)
- Higher-order neighbor에 대한 정보 손실 가능성
- 유연한 학습 불가능: 가중치는 그래프 구조에 고정되어 있음

이에 따라, Velickovic et al. (2018)은 **Learnable Attention Mechanism**을 도입하여, 각 이웃의 상대적 중요도를 직접 학습하도록 설계된 **Graph Attention Networks (GAT)** 을 제안합니다.

---

## Model Architecture

GAT는 self-attention 메커니즘을 통해 **각 노드가 이웃으로부터 받는 정보의 중요도를 학습**합니다. 이 과정은 다음과 같은 주요 단계로 구성됩니다:

### 1. Node Feature Linear Transformation

입력 노드 feature 벡터 $\mathbf{h}_i \in \mathbb{R}^F$에 대해, 선형 변환을 적용하여 임베딩 차원을 정규화합니다:

$$
\mathbf{h}_i^W = W \mathbf{h}_i, \quad W \in \mathbb{R}^{F' \times F}
$$

여기서 $F'$는 변환 후의 차원이며, $W$는 학습 가능한 weight 행렬입니다.

---

### 2. Attention Coefficient Computation

노드 $i$와 이웃 노드 $j$ 사이의 attention score는 다음과 같이 계산됩니다:

$$
e_{ij} = \text{LeakyReLU}\left( \mathbf{a}^\top [\mathbf{h}_i^W \, \| \, \mathbf{h}_j^W] \right)
$$

- $[\cdot \, \| \, \cdot]$는 벡터 연결(concatenation)
- $\mathbf{a} \in \mathbb{R}^{2F'}$: 학습 가능한 벡터
- LeakyReLU의 negative slope: $\alpha = 0.2$

이 수식은 GAT v1 (Velickovic et al., 2018)에서 제안된 방식으로, **각 노드 쌍의 선형변환된 feature를 연결한 후, attention weight vector와 내적하여 score를 계산**합니다.

<figure style="text-align: center; margin: 2em 0;">
    <img
        src = '/assets/img/gat/gat.png'
        alt = "Graph Attention Mechanism"
    >
    <centering>
    <figcaption style="text-align: center;">
        Graph Attention Mechanism
    </figcaption>
    </centering>
</figure>

---

> 📌 **GATv1 vs GATv2**

[HOW ATTENTIVE ARE GRAPH ATTENTION
NETWORKS?(Brody et al., 2021)](https://arxiv.org/pdf/2105.14491)

GATv2는 위 방식의 **표현력 한계**를 극복하기 위해 attention 계산 순서를 다음과 같이 변경합니다:

$$
e_{ij}^{\text{v2}} = \mathbf{a}^\top \cdot \text{LeakyReLU}\left( W[\mathbf{h}_i \, \| \, \mathbf{h}_j] \right)
$$

- 차이점:
  - GAT v1: **선형 변환 후 concat → score**
  - GAT v2: **concat 후 비선형 변환 → score**
- 효과:
  - GATv2는 **입력 순서, 상호작용 표현에 더 민감**하며 **비대칭 attention**도 모델링 가능
  - 학습 가능한 함수 공간이 더 풍부

---

### 3. Softmax Normalization

각 노드는 자신의 이웃 집합 $\mathcal{N}_i$에 대해 attention score를 softmax를 통해 정규화합니다:

$$
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i} \exp(e_{ik})}
$$

이로써, 이웃 노드들이 가지는 상대적 중요도가 확률적으로 표현됩니다.

---

### 4. Neighborhood Feature Aggregation

정규화된 attention weight $\alpha_{ij}$를 기반으로 이웃들의 정보를 가중합:

$$
\mathbf{h}_i' = \sigma\left( \sum_{j \in \mathcal{N}_i} \alpha_{ij} \, \mathbf{h}_j^W \right)
$$

여기서 $\sigma$는 비선형 활성화 함수로 보통 **ELU**를 사용합니다.

---

### 5. Multi-Head Attention

GAT는 안정성과 표현력을 위해 여러 개의 attention head를 동시에 사용합니다:

- **Hidden layer에서는 concat**:

$$
\mathbf{h}_i' = \big\Vert_{k=1}^K \sigma\left( \sum_{j \in \mathcal{N}_i} \alpha_{ij}^{(k)} \, \mathbf{h}_j^{W^{(k)}} \right)
$$

- **Output layer에서는 평균(mean)**:

$$
\mathbf{h}_i' = \frac{1}{K} \sum_{k=1}^K \sigma\left( \sum_{j \in \mathcal{N}_i} \alpha_{ij}^{(k)} \, \mathbf{h}_j^{W^{(k)}} \right)
$$

---

## Advantages of GAT

- **학습 가능한 이웃 가중치**  
  → 중요도가 높은 이웃으로부터 더 많은 정보를 반영

- **입력 그래프 구조에 유연함**  
  → Laplacian 고정 행렬이 불필요, fully data-driven

- **병렬 학습 가능**  
  → CNN처럼 efficient하게 학습 가능

---

## Experiments

논문에서는 4가지 주요 데이터셋을 통해 GAT의 성능을 검증합니다:

| Dataset   | Task         | Accuracy (GAT) |
|-----------|--------------|----------------|
| Cora      | Transductive | 83.0%          |
| Citeseer  | Transductive | 72.5%          |
| Pubmed    | Transductive | 79.0%          |
| PPI       | Inductive    | 0.973 (F1)     |

- Cora/Citeseer: 논문 분류 task
- PPI: 노드의 다중 라벨 예측 (multi-label classification)

---

## Conclusion

Graph Attention Networks는 GNN의 중요한 한계였던 **고정 이웃 평균화의 한계**를 극복하고, **attention 기반의 가중 학습**을 통해 **노드 간 상호작용의 유연한 모델링**을 가능하게 합니다.

---

## Reference

- [Graph Attention Networks](https://arxiv.org/abs/1710.10903).
- [HOW ATTENTIVE ARE GRAPH ATTENTION NETWORKS?](https://arxiv.org/pdf/2105.14491)
