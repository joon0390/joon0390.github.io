---
layout: single  
title: "[Paper Review] Graph Attention Networks"  
date: 2025-05-23  
permalink: /gat/  
categories:
    - Graph Theory
    - Machine Learning    
    - Graph Signal Processing  
tags:  
    - Graph
    - Graph Attention
    - GAT  

toc: true  
toc_sticky: true  
---

> ì´ í¬ìŠ¤íŒ…ì€ Velickovic et al. (2018) ì˜ ë…¼ë¬¸ [Graph Attention Networks](https://arxiv.org/abs/1710.10903)ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.


*ê·¸ë˜í”„ì— ëŒ€í•œ ê°œë…ì€ ë‹¤ìŒì˜ ê¸€ë“¤ì„ ë‹¤ìŒ ê¸€ë“¤ì— ì •ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

- [Graph](/graph/ )
- [Graph Convolutional Networks](/gcn/) 

---

## Introduction

### Motivation

ê¸°ì¡´ì˜ ê·¸ë˜í”„ ì‹ ê²½ë§(Graph Neural Networks, GNN)ë“¤ì€ ì£¼ë¡œ **ê³ ì •ëœ ê°€ì¤‘ì¹˜ ê¸°ë°˜ì˜ ì´ì›ƒ í‰ê· í™”** ë°©ì‹ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ [GCN (Kipf & Welling, 2017)](https://arxiv.org/abs/1609.02907)ì€ ë…¸ë“œì˜ í‘œí˜„ì„ ì£¼ë³€ ì´ì›ƒì˜ featureë¡œë¶€í„° í‰ê· ì ìœ¼ë¡œ aggregateí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ë¥¼ ê°€ì§‘ë‹ˆë‹¤:

- ì´ì›ƒ ë…¸ë“œì˜ ì¤‘ìš”ë„ë¥¼ êµ¬ë¶„í•  ìˆ˜ ì—†ìŒ (ëª¨ë“  ì´ì›ƒì´ ë™ì¼í•œ ë¹„ì¤‘)
- Higher-order neighborì— ëŒ€í•œ ì •ë³´ ì†ì‹¤ ê°€ëŠ¥ì„±
- ìœ ì—°í•œ í•™ìŠµ ë¶ˆê°€ëŠ¥: ê°€ì¤‘ì¹˜ëŠ” ê·¸ë˜í”„ êµ¬ì¡°ì— ê³ ì •ë˜ì–´ ìˆìŒ

ì´ì— ë”°ë¼, Velickovic et al. (2018)ì€ **Learnable Attention Mechanism**ì„ ë„ì…í•˜ì—¬, ê° ì´ì›ƒì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ë¥¼ ì§ì ‘ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„ëœ **Graph Attention Networks (GAT)** ì„ ì œì•ˆí•©ë‹ˆë‹¤.

---

## Model Architecture

GATëŠ” self-attention ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ **ê° ë…¸ë“œê°€ ì´ì›ƒìœ¼ë¡œë¶€í„° ë°›ëŠ” ì •ë³´ì˜ ì¤‘ìš”ë„ë¥¼ í•™ìŠµ**í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

### 1. Node Feature Linear Transformation

ì…ë ¥ ë…¸ë“œ feature ë²¡í„° $\mathbf{h}_i \in \mathbb{R}^F$ì— ëŒ€í•´, ì„ í˜• ë³€í™˜ì„ ì ìš©í•˜ì—¬ ì„ë² ë”© ì°¨ì›ì„ ì •ê·œí™”í•©ë‹ˆë‹¤:

$$
\mathbf{h}_i^W = W \mathbf{h}_i, \quad W \in \mathbb{R}^{F' \times F}
$$

ì—¬ê¸°ì„œ $F'$ëŠ” ë³€í™˜ í›„ì˜ ì°¨ì›ì´ë©°, $W$ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ weight í–‰ë ¬ì…ë‹ˆë‹¤.

---

### 2. Attention Coefficient Computation

ë…¸ë“œ $i$ì™€ ì´ì›ƒ ë…¸ë“œ $j$ ì‚¬ì´ì˜ attention scoreëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤:

$$
e_{ij} = \text{LeakyReLU}\left( \mathbf{a}^\top [\mathbf{h}_i^W \, \| \, \mathbf{h}_j^W] \right)
$$

- $[\cdot \, \| \, \cdot]$ëŠ” ë²¡í„° ì—°ê²°(concatenation)
- $\mathbf{a} \in \mathbb{R}^{2F'}$: í•™ìŠµ ê°€ëŠ¥í•œ ë²¡í„°
- LeakyReLUì˜ negative slope: $\alpha = 0.2$

ì´ ìˆ˜ì‹ì€ GAT v1 (Velickovic et al., 2018)ì—ì„œ ì œì•ˆëœ ë°©ì‹ìœ¼ë¡œ, **ê° ë…¸ë“œ ìŒì˜ ì„ í˜•ë³€í™˜ëœ featureë¥¼ ì—°ê²°í•œ í›„, attention weight vectorì™€ ë‚´ì í•˜ì—¬ scoreë¥¼ ê³„ì‚°**í•©ë‹ˆë‹¤.

<figure style="text-align: center; margin: 2em 0;">
    <img
        src = '/assets/img/gat/gat.png'
        alt = "Graph Attention Mechanism"
    >
    <centering>
    <figcaption style="text-align: center;">
        Graph Attention Mechanism
    </figcaption>
    </centering>
</figure>

---

> ğŸ“Œ **GATv1 vs GATv2**

[HOW ATTENTIVE ARE GRAPH ATTENTION
NETWORKS?(Brody et al., 2021)](https://arxiv.org/pdf/2105.14491)

GATv2ëŠ” ìœ„ ë°©ì‹ì˜ **í‘œí˜„ë ¥ í•œê³„**ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ attention ê³„ì‚° ìˆœì„œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½í•©ë‹ˆë‹¤:

$$
e_{ij}^{\text{v2}} = \mathbf{a}^\top \cdot \text{LeakyReLU}\left( W[\mathbf{h}_i \, \| \, \mathbf{h}_j] \right)
$$

- ì°¨ì´ì :
  - GAT v1: **ì„ í˜• ë³€í™˜ í›„ concat â†’ score**
  - GAT v2: **concat í›„ ë¹„ì„ í˜• ë³€í™˜ â†’ score**
- íš¨ê³¼:
  - GATv2ëŠ” **ì…ë ¥ ìˆœì„œ, ìƒí˜¸ì‘ìš© í‘œí˜„ì— ë” ë¯¼ê°**í•˜ë©° **ë¹„ëŒ€ì¹­ attention**ë„ ëª¨ë¸ë§ ê°€ëŠ¥
  - í•™ìŠµ ê°€ëŠ¥í•œ í•¨ìˆ˜ ê³µê°„ì´ ë” í’ë¶€

---

### 3. Softmax Normalization

ê° ë…¸ë“œëŠ” ìì‹ ì˜ ì´ì›ƒ ì§‘í•© $\mathcal{N}_i$ì— ëŒ€í•´ attention scoreë¥¼ softmaxë¥¼ í†µí•´ ì •ê·œí™”í•©ë‹ˆë‹¤:

$$
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i} \exp(e_{ik})}
$$

ì´ë¡œì¨, ì´ì›ƒ ë…¸ë“œë“¤ì´ ê°€ì§€ëŠ” ìƒëŒ€ì  ì¤‘ìš”ë„ê°€ í™•ë¥ ì ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.

---

### 4. Neighborhood Feature Aggregation

ì •ê·œí™”ëœ attention weight $\alpha_{ij}$ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ì›ƒë“¤ì˜ ì •ë³´ë¥¼ ê°€ì¤‘í•©:

$$
\mathbf{h}_i' = \sigma\left( \sum_{j \in \mathcal{N}_i} \alpha_{ij} \, \mathbf{h}_j^W \right)
$$

ì—¬ê¸°ì„œ $\sigma$ëŠ” ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¡œ ë³´í†µ **ELU**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

---

### 5. Multi-Head Attention

GATëŠ” ì•ˆì •ì„±ê³¼ í‘œí˜„ë ¥ì„ ìœ„í•´ ì—¬ëŸ¬ ê°œì˜ attention headë¥¼ ë™ì‹œì— ì‚¬ìš©í•©ë‹ˆë‹¤:

- **Hidden layerì—ì„œëŠ” concat**:

$$
\mathbf{h}_i' = \big\Vert_{k=1}^K \sigma\left( \sum_{j \in \mathcal{N}_i} \alpha_{ij}^{(k)} \, \mathbf{h}_j^{W^{(k)}} \right)
$$

- **Output layerì—ì„œëŠ” í‰ê· (mean)**:

$$
\mathbf{h}_i' = \frac{1}{K} \sum_{k=1}^K \sigma\left( \sum_{j \in \mathcal{N}_i} \alpha_{ij}^{(k)} \, \mathbf{h}_j^{W^{(k)}} \right)
$$

---

### 6. Algorithm Summary

1. ë…¸ë“œ featureì— ì„ í˜• ë³€í™˜ ì ìš©
2. ì´ì›ƒ ë…¸ë“œì™€ concatí•˜ì—¬ attention score ê³„ì‚°
3. softmaxë¡œ ì •ê·œí™”í•˜ì—¬ attention weight \(\alpha_{ij}\) ê³„ì‚°
4. ì´ì›ƒ featureë“¤ì„ attention ê¸°ë°˜ ê°€ì¤‘í•©
5. ì—¬ëŸ¬ headë¥¼ ì‚¬ìš©í•˜ì—¬ concat ë˜ëŠ” í‰ê·  ì²˜ë¦¬

---

## Advantages of GAT

- **í•™ìŠµ ê°€ëŠ¥í•œ ì´ì›ƒ ê°€ì¤‘ì¹˜**  
  â†’ ì¤‘ìš”ë„ê°€ ë†’ì€ ì´ì›ƒìœ¼ë¡œë¶€í„° ë” ë§ì€ ì •ë³´ë¥¼ ë°˜ì˜

- **ì…ë ¥ ê·¸ë˜í”„ êµ¬ì¡°ì— ìœ ì—°í•¨**  
  â†’ Laplacian ê³ ì • í–‰ë ¬ì´ ë¶ˆí•„ìš”, fully data-driven

- **ë³‘ë ¬ í•™ìŠµ ê°€ëŠ¥**  
  â†’ CNNì²˜ëŸ¼ efficientí•˜ê²Œ í•™ìŠµ ê°€ëŠ¥

---

## Code

íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•œ ê°„ë‹¨í•œ GAT ì½”ë“œì…ë‹ˆë‹¤.

```python
import torch
import torch.nn.functional as F
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import GATConv

# 1. ë°ì´í„°ì…‹ ë¡œë“œ
dataset = Planetoid(root='data/Cora', name='Cora')
data = dataset[0]

# 2. GAT ëª¨ë¸ ì •ì˜
class GAT(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, heads=8, dropout=0.6):
        super().__init__()
        self.gat1 = GATConv(in_dim, hidden_dim, heads=heads, dropout=dropout)
        self.gat2 = GATConv(hidden_dim * heads, out_dim, heads=1, concat=False, dropout=dropout)
        self.dropout = dropout

    def forward(self, x, edge_index):
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = F.elu(self.gat1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.gat2(x, edge_index)
        return F.log_softmax(x, dim=1)

# 3. Training setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GAT(in_dim=dataset.num_node_features, hidden_dim=8, out_dim=dataset.num_classes).to(device)
data = data.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)

# 4. Training ë£¨í”„
def train():
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()
    return loss.item()

# 5. Evaluation
def test():
    model.eval()
    out = model(data.x, data.edge_index)
    pred = out.argmax(dim=1)
    accs = []
    for mask in [data.train_mask, data.val_mask, data.test_mask]:
        acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()
        accs.append(acc)
    return accs

# 6. Training
for epoch in range(1, 201):
    loss = train()
    train_acc, val_acc, test_acc = test()
    if epoch % 20 == 0:
        print(f"Epoch {epoch:03d} | Loss: {loss:.4f} | Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}")
        
```

---

## Conclusion

Graph Attention NetworksëŠ” GNNì˜ ì¤‘ìš”í•œ í•œê³„ì˜€ë˜ **ê³ ì • ì´ì›ƒ í‰ê· í™”ì˜ í•œê³„**ë¥¼ ê·¹ë³µí•˜ê³ , **attention ê¸°ë°˜ì˜ ê°€ì¤‘ í•™ìŠµ**ì„ í†µí•´ **ë…¸ë“œ ê°„ ìƒí˜¸ì‘ìš©ì˜ ìœ ì—°í•œ ëª¨ë¸ë§**ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

---

## Reference

- [Graph Attention Networks](https://arxiv.org/abs/1710.10903).
- [HOW ATTENTIVE ARE GRAPH ATTENTION NETWORKS?](https://arxiv.org/pdf/2105.14491)
