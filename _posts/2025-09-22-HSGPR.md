---
layout: single  
title: "[Paper Review] Hilbert Space Methods for Reduced-Rank Gaussian Process Regression"  
date: 2025-09-22  
permalink: /HSGPR/  
categories:  
  - Statistics
  - Machine Learning
tags:  
  - Gaussian Process Regression
  - Gaussian Process
  - Hilbert Space
  - Low-Rank Approximation

toc: true  
toc_sticky: true  
---

> 이 포스팅은 A. Solin et al. (2014)의 논문 [Hilbert Space Methods for Reduced-Rank Gaussian Process Regression](https://arxiv.org/pdf/1401.5508)을 읽고 정리한 글입니다.

---

## Introduction

[Gaussian Process Regression](/gp/)은 Nonparametric Bayesian Regression 기법으로, 불확실성 추정과 유연한 함수 근사가 가능하다는 장점 때문에 널리 사용됩니다. 
하지만 GPR의 가장 큰 단점은 **계산 복잡도**입니다. 학습과 예측 모두에서 $O(n^3)$의 연산이 필요하여, 데이터 수 $n$이 커지면 실질적인 사용이 어려워집니다. 

이 문제를 해결하기 위해 Sparse GP 기법들이 제안되어왔으며, 그 중에서도 대표적으로 **Inducing Points 기반 근사**가 많이 사용됩니다. 



### 📎 Sparse GP & Inducing Points

- **Sparse GP**: Gaussian Process의 $O(n^3)$ 연산을 줄이기 위해 저차원 근사를 사용하는 방법론의 총칭.  
- **Inducing Points**: Sparse GP의 대표 기법으로, $m \ll n$개의 보조 변수(inducing variables)를 사용해 커널 행렬을 근사.  
  $$
  K_{nn} \approx K_{nm} K_{mm}^{-1} K_{mn}
  $$  
- **장점**: 효율적이며, 대규모 데이터셋에도 적용 가능.  
- **한계**: inducing points 선택에 따라 성능 차이가 크고, 근사 오차가 발생.  


---

## Key Ieda

핵심이 되는 아이디어는 다음과 같습니다. 
- GP의 공분산 커널 $k(x,x')$를 라플라스 연산자의 고유함수(eigen function) Basis로 전개합니다.
- 무한 급수를 유한 개의 항으로 잘라서 근사하면, 자연스럽게 Low Rank Approximation이 이루어집니다. 

수식적으로,

$$k(x,x') \approx \sum_{j=1}^m\lambda_j\phi_j(x)\phi_j(x')$$

- $\phi_j$ 는 고유함수, $\lambda_j$는 고유함수에 대응하는 고유값

즉, 커널을 직접 계산하는 대신, 미리 계산된 기저 함수를 이용하여 표현한다고 볼 수 있습니다. 

### 📎 고유함수란?

#### 선형대수에서의 고유벡터
행렬 $A$가 있을 때, $A\nu = \lambda \nu$ 를 만족하는 $\nu$를 고유벡터(eigenvector), $\lambda$를 고유값(eigenvalue)라고 부릅니다.  
즉 특정한 벡터 $\nu$는 행렬 $A$를 곱해도 방향은 바뀌지 않고 크기만 $\lambda$ 배 달라집니다.  

#### 연산자의 고유함수
행렬 대신 **연산자(ex. 미분, 적분, 라플라시안)** 를 고려해봅시다.  
예를 들어, 미분 연산자 $\tfrac{d}{dx}$를 함수 $f(x)$에 적용하면:
$$
\frac{d}{dx} f(x) = \lambda f(x)
$$
이 조건을 만족시키는 함수 $f(x)$를 **고유함수**라고 하고, $\lambda$을 고유값이라고 합니다.  
예: $f(x) = e^{\lambda x}$ 는 미분 연산자의 고유함수입니다.  


---

## Method

핵심 아이디어는 **커널을 Hilbert 공간의 고유함수 전개로 근사**하는 것입니다.  

> 📎 **Hilbert Space란?**  
> 단순히 함수들의 집합이 아니라  
> $$
> \langle f,g \rangle = \int_{\Omega} f(x)g(x)\,dx
> $$  
> 같은 **내적(inner product)** 이 정의된 함수 공간을 말합니다.  
> 내적이 있기 때문에 “직교(orthogonality)” 개념을 쓸 수 있고,  
> 이 덕분에 **고유함수들이 직교 기저(orthogonal basis)** 를 이루게 됩니다.  

---

### 1. Laplacian Eigenfunction Decomposition
도메인 $\Omega$ 위에서 라플라스 연산자의 고유문제를 풅니다:

$$
-\Delta \phi_j(x) = \gamma_j \phi_j(x), \quad x \in \Omega \qquad (a)
$$

- 1차원에서 라플라스 연산자는 단순히 **2차 미분**: $\Delta f = f''$.  
- 예: $\phi(x) = \sin(\pi x)$  
  - $\phi''(x) = -\pi^2 \sin(\pi x)$  
  - 따라서 $\Delta \phi(x) = -\pi^2 \phi(x)$  
  - 고유값은 $-\pi^2$ (음수)  
  - 식 (a)처럼 $-\Delta$를 쓰면 고유값이 **양수 $\gamma=\pi^2$** 로 바뀌어 주파수 제곱으로 해석할 수 있습니다.  

- Dirichlet 경계조건 → **사인 함수 기저**  
- Neumann 경계조건 → **코사인 함수 기저**  

즉, **도메인과 경계조건에 따라 Hilbert 공간 $L^2(\Omega)$ 위의 직교 기저 $\{\phi_j\}$가 결정**됩니다.  

---

### 2. Spectral Density와 고유값 연결
[Bochner 정리](https://en.wikipedia.org/wiki/Bochner's_theorem)에 따라 정상 커널 $k(x,x')$는 스펙트럴 밀도 $S(\omega)$의 푸리에 변환으로 표현됩니다:
$$
k(\tau) = \int_{\mathbb{R}^d} e^{i\omega^\top \tau}\, S(\omega)\, d\omega, \quad \tau=x-x'.
$$

라플라스 고유값 $\gamma_j$는 $\omega_j=\sqrt{\gamma_j}$라는 “주파수”로 해석할 수 있습니다.  
따라서 각 기저의 weight는
$$
\lambda_j \;\approx\; S(\sqrt{\gamma_j}) \cdot w_j
$$
로 결정됩니다.  
- $\lambda_j$: 고유함수에 붙는 가중치  
- $w_j$: 수치적분(quadrature) 보정값  

즉, **기저는 라플라스가 정하고, 가중치는 커널 스펙트럼이 정하는 것**입니다.  

---

### 3. Low-Rank Approximation
무한 전개를 상위 $m$개까지만 잘라 근사합니다:
$$
k(x,x') \approx \sum_{j=1}^m \lambda_j \phi_j(x)\phi_j(x').
$$

행렬 형태:
$$
K \approx \Phi \Lambda \Phi^\top, \qquad
\Phi_{ij}=\phi_j(x_i), \quad
\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_m).
$$

즉, 무한 차원의 커널 연산자를 **rank-$m$ 근사**로 바꿔치기 한 것입니다.  

---

### 4. Posterior Inference
근사 커널을 사용하면 GP 추론은 feature-space에서 선형 모델로 바뀝니다:
$$
f(x) \approx \phi(x)^\top w, \quad w \sim \mathcal{N}(0,\Lambda).
$$
$$
y = f(x) + \epsilon, \quad \epsilon \sim \mathcal{N}(0,\sigma_n^2 I).
$$

Posterior 분포는 정규분포이며,  
$A = \Lambda + \tfrac{1}{\sigma_n^2}\Phi^\top \Phi$라 두면:

- **Posterior mean**
$$
\mu(x_*) = \phi(x_*)^\top \Lambda A^{-1}\,\tfrac{1}{\sigma_n^2}\Phi^\top y
$$

- **Posterior covariance**
$$
\mathrm{cov}(f(x_*)) = \phi(x_*)^\top \Lambda \phi(x_*) \;-\; \phi(x_*)^\top \Lambda A^{-1}\Lambda \phi(x_*).
$$

---

### 5. Computational Complexity
- Exact GP: $O(n^3)$  
- Hilbert-space RR-GP: $O(nm^2 + m^3)$  

따라서 $m \ll n$일 때 큰 계산 효율을 얻을 수 있습니다.  

---

✅ 정리하면,  
1) Hilbert 공간에서 라플라스 고유함수 기저를 구하고,  
2) 커널 스펙트럼을 이용해 각 기저에 weight를 부여하며,  
3) 무한 전개를 rank-$m$ 근사로 자르고,  
4) feature-space에서 posterior를 계산함으로써,  
**Exact GP를 효율적으로 근사**할 수 있습니다.

---

## Experiments

논문/슬라이드에서는 제안한 Hilbert-space Reduced-Rank GP(RR-GP)를 다양한 상황에서 실험했습니다.

- **1D 토이 예제**
  - 주기 함수를 관측치에 피팅.
  - Exact GP, Sparse GP (SOR, DTC, FIC 등)와 비교.
  - RR-GP는 rank $m$이 커질수록 RMSE가 낮아지며, Evidence(Marginal Likelihood)도 Exact GP에 근접.

<br>

- **2D 상관 구조 비교**
  - 제안한 Laplacian Hilbert 기저 근사와 Sparse Spectrum, Inducing Points 기반 근사를 비교.
  - Exact GP의 correlation contour(등고선)와 RR-GP 근사가 잘 일치함.
  - 특히 Laplacian 기저 기반은 Inducing points 랜덤 배치보다 안정적인 근사 성능.

<br>

- **실제 데이터셋**
  - **Precipitation data (n=5776)**  
    - 연간 강수량 데이터를 GP로 보간.  
    - Full GP 대비 RR-GP가 훨씬 빠른 계산 속도를 보이며, 정확도 손실은 미미.  
    - Sparse GP 방법들보다도 Marginal Likelihood와 RMSE가 더 안정적임.  

  - **Global surface temperature (n=11028)**  
    - 전 지구 표면 온도 데이터에 대해 GP 회귀 수행.  
    - Matérn 커널(ν = 1/2, 3/2, ∞)로 샘플링된 결과가 RR-GP 근사에서도 Exact GP와 유사하게 나타남.  
    - 대규모 데이터에서도 계산 효율과 근사 정확도를 동시에 확보.

---

## Conclusion

- Hilbert-space RR-GP는 **Laplacian 고유함수 기저 + 커널 스펙트럼 기반 고유값**을 사용해 커널을 근사.  
- 이 방식은 **데이터에 의존하지 않는 안정적 기저**를 제공하며, Inducing point 선택의 불안정성을 피할 수 있음.  
- 근사 정확도는 rank $m$을 늘릴수록 Exact GP에 수렴하고, Marginal Likelihood 기반 하이퍼파라미터 추정이 안정적으로 가능.  
- 대규모 데이터셋에서도 Full GP 대비 큰 계산 효율 향상을 보이며, Sparse GP와 비교해도 더 안정적인 근사 결과를 제공.  

---

## References

- [*Hilbert Space Methods for Reduced-Rank Gaussian Process Regression*.](https://arxiv.org/pdf/1401.5508)  

- [Workshop on Gaussian Process Approximation](https://gpss.cc/gpa15/assets/sarkka.pdf)

