---
layout: single
title: "Bayesian PCA"
date: 2025-09-30
permalink: /bpca/
categories:
  - Statistics
  - Bayesian
tags:
  - Dimensionality Reduction
  - PCA
  - PPCA
  - BPCA

toc: true
toc_sticky: true
comments: true
---

> 이 포스팅은 Bayesian Principle Component Analysis(BPCA)에 대해 공부하고 정리한 글입니다.  



## Introduction
- 고전 PCA에서 시작하여 **Probabilistic PCA (PPCA)**, 그리고 **Bayesian PCA (BPCA)** 로 이어지는 과정을 정리합니다.  
- PCA → PPCA → BPCA 순으로 **모델링 관점의 변화**를 살펴보고, 왜 Bayesian 접근이 필요한지 설명합니다.  


## 1. PCA 복습
- PCA의 목적은 고차원 데이터 $X \in \mathbb{R}^{d \times N}$를 저차원 잠재 변수 $Z \in \mathbb{R}^{q \times N}, q<<d $로 선형 변환하는 것입니다.  
- 수학적 정의는 Covariance Matrix의 Eigen Decomposition입니다.  
- **한계**: 단순 선형 변환이며 확률 모델이 아니므로 불확실성 추정이나 결측 데이터 처리는 어렵습니다.  


## 2. Probabilistic PCA (PPCA)
- [Tipping & Bishop (1999)](https://www.di.ens.fr/~fbach/courses/fall2005/Bishop_Tipping_1999_Probabilistic_PCA.pdf)에서 제안되었습니다.  
- **모델**:
  $$
  x = W z + \mu + \epsilon, \quad 
  z \sim \mathcal{N}(0, I_q), \quad
  \epsilon \sim \mathcal{N}(0, \sigma^2 I_d)
  $$
- 따라서 $x \sim \mathcal{N}(\mu, W W^\top + \sigma^2 I)$로 표현됩니다.  
- 장점: 확률적 모델로서 MLE/EM 알고리즘을 적용할 수 있으며, 결측치 처리에도 유리합니다.  


## 3. Bayesian PCA (BPCA)
- Bishop (1999)에 의해 제안되었습니다.  
- 파라미터를 고정값이 아닌 **확률 분포**로 두어 Bayesian 추정을 수행합니다.
  - $w_i \sim \mathcal{N}(0, \alpha_i^{-1} I)$  
  - $\alpha_i \sim \text{Gamma}(a_\alpha, b_\alpha)$  
  - $\tau \sim \text{Gamma}(a_\tau, b_\tau)$  
- 특징:
  1. **Automatic Relevance Determination (ARD)**를 통해 유효한 잠재 차원만 남길 수 있습니다.  
  2. **Overfitting 방지** 효과를 제공합니다.  
  3. **Posterior 불확실성 추정**이 가능합니다.  


## 4. Variational Inference in BPCA
- Posterior 분포 $p(W, Z, \mu, \alpha, \tau | X)$는 계산하기 어렵습니다. 따라서 **변분추론(VB)**을 사용합니다.  
- 분리 가정은 다음과 같습니다:
  $$
  q(W, Z, \mu, \alpha, \tau) = q(W)q(Z)q(\mu)q(\alpha)q(\tau)
  $$
- **업데이트 방정식**:
  - $q(z)$: 다변량 정규분포  
  - $q(W)$: 다변량 정규분포  
  - $q(\alpha), q(\tau)$: 감마 분포  
- ELBO (Evidence Lower Bound)를 최적화하면서 수렴합니다.  


## 5. 핵심 수식 요약
1. Posterior of $z$:
   $$
   q(z) = \mathcal{N}\!\left( \tau (I + \tau W^\top W)^{-1} W^\top (x-\mu), \, (I + \tau W^\top W)^{-1} \right)
   $$
2. Posterior of $W$: 정규분포, ARD prior 반영  
3. Posterior of $\alpha, \tau$: 감마 분포로 업데이트  


## 6. 장점과 단점
**장점**  
- 차원을 자동으로 선택할 수 있습니다.  
- Overfitting을 방지할 수 있습니다.  
- 결측치를 처리할 수 있습니다.  

**단점**  
- 수치적 안정성 문제가 발생할 수 있습니다 (역행렬, determinant 계산 등).  
- Mini-batch 적용이 까다롭습니다.  
- 대규모 데이터에서는 GPLVM, VAE가 더 자주 사용됩니다.  


## 7. 실습 아이디어
- 작은 데이터셋 (예: MNIST, Iris)을 대상으로 PCA, PPCA, BPCA를 비교합니다.  
- 시각화를 통해 latent 2D embedding을 비교합니다.  
- $\alpha$ 값의 크기를 확인하여 차원이 자동으로 줄어드는지 살펴봅니다.  


## Conclusion
- PCA → PPCA → BPCA는 **통계적 차원축소의 자연스러운 확장 과정**입니다.  
- BPCA는 Bayesian 접근을 통해 불확실성과 차원 자동 선택 기능을 제공합니다.  
- 실제 응용에서는 GPLVM, VAE 같은 딥러닝 기반 확장이 뒤따릅니다.  


## Reference
- Tipping, M. E., & Bishop, C. M. (1999). *Probabilistic Principal Component Analysis*. J. Royal Statistical Society B.  
- Bishop, C. M. (1999). *Bayesian PCA*. NIPS.  
