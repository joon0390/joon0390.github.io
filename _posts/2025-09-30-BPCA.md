---
layout: single
title: "Bayesian PCA"
date: 2025-09-30
permalink: /bpca/
categories:
  - Statistics
  - Bayesian
tags:
  - Dimensionality Reduction
  - PCA
  - PPCA
  - BPCA

toc: true
toc_sticky: true
comments: true
---

> 이 포스팅은 Bayesian Principal Component Analysis (BPCA)에 대해 공부하고 정리한 글입니다.  



## Introduction
- 고전 PCA에서 시작하여 **Probabilistic PCA (PPCA)**, 그리고 **Bayesian PCA (BPCA)** 로 이어지는 과정을 정리합니다.  
- PCA → PPCA → BPCA 순으로 **모델링 관점의 변화**를 살펴보고, 왜 Bayesian 접근이 필요한지 설명합니다.  


## 1. PCA 
- PCA의 목적은 고차원 데이터 $X \in \mathbb{R}^{d \times N}$를 저차원 잠재 변수 $Z \in \mathbb{R}^{q \times N}, q \ll d$로 선형 변환하는 것입니다.  
- 수학적 정의는 Covariance Matrix의 Eigen Decomposition입니다.  
- **한계**: 단순 선형 변환이며 확률 모델이 아니므로 불확실성 추정이나 결측 데이터 처리는 어렵습니다.  


## 2. Probabilistic PCA (PPCA)
- [Tipping & Bishop (1999)](https://www.di.ens.fr/~fbach/courses/fall2005/Bishop_Tipping_1999_Probabilistic_PCA.pdf)에서 제안되었습니다.  

> [PPCA](/ppca/) 포스팅을 참고해주세요


## 3. Bayesian PCA (BPCA)
- Bishop (1999)에 의해 제안되었습니다.  
- 파라미터를 고정값이 아닌 **확률 분포**로 두어 Bayesian 추정을 수행합니다.
  - $w_i \sim \mathcal{N}(0, \alpha_i^{-1} I)$  
  - $\alpha_i \sim \text{Gamma}(a_\alpha, b_\alpha)$  
  - $\tau \sim \text{Gamma}(a_\tau, b_\tau)$  
- 특징:
  1. **Automatic Relevance Determination (ARD)** 를 통해 유효한 잠재 차원만 남길 수 있습니다.  
  2. **Overfitting 방지** 효과를 제공합니다.  
  3. **Posterior 불확실성 추정**이 가능합니다.  


## 4. Methodology

### 4.1 Generative Model
BPCA는 다음과 같은 확률적 그래픽 모델로 표현됩니다.

- Latent variables:  
  $z_n \sim \mathcal{N}(0, I_q)$  

- Observed data:  
  $x_n | z_n, W, \mu, \tau \sim \mathcal{N}(W z_n + \mu, \tau^{-1} I_d)$  

- Weight prior (column-wise):  
  $w_i \sim \mathcal{N}(0, \alpha_i^{-1} I)$  

- Hyperpriors:  
  $\alpha_i \sim \text{Gamma}(a_\alpha, b_\alpha), \quad \tau \sim \text{Gamma}(a_\tau, b_\tau)$  

따라서 전체 joint 분포는 다음과 같습니다.

$$
p(X,Z,W,\alpha,\tau) = p(W|\alpha)\,p(\alpha)\,p(\tau)\,\prod_{n=1}^N p(z_n)\,p(x_n|z_n,W,\tau)
$$

---

### 4.2 Variational Inference
Posterior $p(W,Z,\mu,\alpha,\tau|X)$는 직접 계산이 어렵습니다.  
따라서 **변분추론(Variational Bayes)**으로 근사합니다.

- 분리 가정:
  $$
  q(W,Z,\mu,\alpha,\tau) = q(W)\,q(Z)\,q(\mu)\,q(\alpha)\,q(\tau)
  $$

- 각 분포 형태:
  - $q(z_n) = \mathcal{N}(\cdot)$ (정규분포)  
  - $q(W) = \mathcal{N}(\cdot)$ (정규분포)  
  - $q(\alpha), q(\tau)$ = 감마 분포  

---

### 4.3 Update Equations
1. **Posterior of $z_n$**  
   $$
   q(z_n) = \mathcal{N}\!\left( \tau (I + \tau W^\top W)^{-1} W^\top (x_n-\mu), \, (I + \tau W^\top W)^{-1} \right)
   $$

2. **Posterior of $W$**  
   - 정규분포 형태, ARD prior가 반영되어 각 열이 shrinkage 효과를 가짐.

3. **Posterior of $\alpha, \tau$**  
   - 감마 분포로 업데이트되며, $\alpha$가 커지면 해당 $w_i$가 0에 가까워짐.  
   - 이는 곧 **Automatic Relevance Determination (ARD)** 효과를 발생시켜, 불필요한 latent 차원을 제거합니다.

---

### 4.4 ELBO Optimization
- 각 step에서 Evidence Lower Bound (ELBO)를 계산하고,  
- 이를 최대화하는 방향으로 파라미터와 분포를 반복적으로 업데이트합니다.  
- 수렴 시 근사 posterior가 최종 추정치가 됩니다.

---

## 5. Key formula summary
1. Posterior of $z$:
   $$
   q(z) = \mathcal{N}\!\left( \tau (I + \tau W^\top W)^{-1} W^\top (x-\mu), \, (I + \tau W^\top W)^{-1} \right)
   $$
2. Posterior of $W$: 정규분포, ARD prior 반영  
3. Posterior of $\alpha, \tau$: 감마 분포로 업데이트  


## Conclusion
- PCA → PPCA → BPCA는 **통계적 차원축소의 자연스러운 확장 과정**입니다.  
- BPCA는 Bayesian 접근을 통해 불확실성과 차원 자동 선택 기능을 제공합니다.  
- ARD를 통해 **불필요한 잠재 차원을 스스로 제거**한다는 점이 가장 큰 차별점입니다.  
- 하지만 수치적 안정성과 계산 복잡도 문제로 인해, 대규모 데이터에서는 **GPLVM, VAE** 같은 후속 기법이 더 자주 활용됩니다.  


## Reference
- [Bishop, C. M. (1999). *Bayesian PCA*. NIPS.](https://proceedings.neurips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf)  
- [Github - MaxenceGiraud/BayesianPCA](https://github.com/MaxenceGiraud/BayesianPCA)