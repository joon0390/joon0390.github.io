---
layout: single
title: "Mixture of Experts (MoE)"
date: 2025-09-28
permalink: /moe/
categories:
  - Deep Learning
  - Machine Learning
tags:
  - Mixture of Experts
  - MoE
  - Ensemble
  - Neural Networks
toc: true
toc_sticky: true
comments: true
---

> ì´ í¬ìŠ¤íŒ…ì€ Mixture of Experts(MoE)ì˜ ê°œë…ì„ ì •ë¦¬í•˜ê³ , ì „í†µì ì¸ Ensembleê³¼ ë¹„êµí•˜ì—¬ ì°¨ì´ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## Introduction

ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ê°•ë ¥í•œ ë°©ë²• ì¤‘ í•˜ë‚˜ëŠ” **ì—¬ëŸ¬ ëª¨ë¸ì„ ì¡°í•©í•˜ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.  
ëŒ€í‘œì ì¸ ë°©ì‹ì´ **ì•™ìƒë¸”(Ensemble)** ì´ê³ , ë˜ ë‹¤ë¥¸ ë°©ì‹ì´ **Mixture of Experts(MoE)** ì…ë‹ˆë‹¤.  

ì´ ë‘˜ì€ "ì—¬ëŸ¬ ëª¨ë¸ì„ ì“´ë‹¤"ëŠ” ì ì—ì„œëŠ” ë¹„ìŠ·í•˜ì§€ë§Œ,  
**ë™ì‘ ë°©ì‹ê³¼ ëª©í‘œëŠ” ì™„ì „íˆ ë‹¤ë¦…ë‹ˆë‹¤.**

---

## Mixture of Experts (MoE)

MoEëŠ” í¬ê²Œ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1. **Experts**  
   - ì„œë¡œ ë‹¤ë¥¸ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ì „ë¬¸ê°€ ëª¨ë¸.  
   - ì˜ˆ: $f_1(x), f_2(x), \dots, f_M(x)$  

2. **Gating Network**  
   - ì…ë ¥ $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì–´ë–¤ Expertë¥¼ ì“¸ì§€ ê²°ì •í•˜ëŠ” ëª¨ë“ˆ.  
   - ë³´í†µ Softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì‚°ì¶œ.  

ìµœì¢… ì¶œë ¥ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤.

$$
y(x) = \sum_{i=1}^M g_i(x) \, f_i(x)
$$

- $f_i(x)$ : $i$ë²ˆì§¸ Expertì˜ ì¶œë ¥  
- $g_i(x)$ : ê²Œì´íŠ¸ê°€ ê²°ì •í•œ ê°€ì¤‘ì¹˜ (ì…ë ¥ ì¡°ê±´ë¶€)  

ì¦‰, **ì…ë ¥ì— ë”°ë¼ Expert ì„ íƒì´ ë‹¬ë¼ì§„ë‹¤**ëŠ” ì ì´ MoEì˜ í•µì‹¬ì…ë‹ˆë‹¤.
![Visualization of MOE Architecture](/assets/img/moe/moe.png)
ì¶œì²˜: [Link](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

---

### ExpertëŠ” ì‹ ê²½ë§ì— êµ­í•œë˜ì§€ ì•ŠëŠ”ë‹¤

MoEë¼ëŠ” ì´ë¦„ ë•Œë¬¸ì— í”íˆ "ì—¬ëŸ¬ ê°œì˜ ì‹ ê²½ë§ì„ ë‘ëŠ” êµ¬ì¡°"ë¼ê³  ìƒê°í•˜ê¸° ì‰½ì§€ë§Œ,  
ì‹¤ì œë¡œ **ExpertëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ëª¨ë¸ì´ë“  ê°€ëŠ¥**í•©ë‹ˆë‹¤.  

- **í†µê³„ ëª¨ë¸**: ì„ í˜• íšŒê·€, ë¡œì§€ìŠ¤í‹± íšŒê·€, Gaussian Process  
- **íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸**: Decision Tree, Random Forest, Gradient Boosting  
- **ì „í†µ ML ê¸°ë²•**: SVM, kNN, Naive Bayes  
- **ê·œì¹™ ê¸°ë°˜(rule-based) ëª¨ë“ˆ**ë„ Expertê°€ ë  ìˆ˜ ìˆìŒ  

ì¤‘ìš”í•œ ê±´ **ê²Œì´íŒ… ë„¤íŠ¸ì›Œí¬ê°€ ì…ë ¥ ì¡°ê±´ì— ë”°ë¼ Expertë¥¼ ê³¨ë¼ì£¼ëŠ” êµ¬ì¡°**ì´ì§€, Expert ìì²´ì˜ í˜•íƒœëŠ” ììœ ë¡­ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.  

ğŸ“Œ ì°¸ê³ : ì´ˆê¸° MoE ë…¼ë¬¸[(Jordan & Jacobs, 1994)](https://ieeexplore.ieee.org/document/381768)ì—ì„œë„ ExpertëŠ” ë‹¨ìˆœí•œ **ì„ í˜• íšŒê·€ ëª¨ë¸**ì´ì—ˆìœ¼ë©°,  
í˜„ëŒ€ ë”¥ëŸ¬ë‹ì—ì„œëŠ” ì‹ ê²½ë§ Expertê°€ ì£¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. 

---

## Ensemble

Ensembleì€ ì—¬ëŸ¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ í›„,  
ì˜ˆì¸¡ ë‹¨ê³„ì—ì„œ **ëª¨ë“  ëª¨ë¸ì˜ ì¶œë ¥ì„ í‰ê· í•˜ê±°ë‚˜ íˆ¬í‘œ**í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

$$
y(x) = \frac{1}{M} \sum_{i=1}^M f_i(x)
$$

- ëª¨ë“  ì…ë ¥ì´ **ëª¨ë“  ëª¨ë¸ì„ ë™ì¼í•˜ê²Œ í†µê³¼**í•©ë‹ˆë‹¤.  
- ê²Œì´íŠ¸ ê°™ì€ ì¡°ê±´ë¶€ ì„ íƒì€ ì—†ê³ , ë‹¨ìˆœíˆ ì „ì²´ ê²°ê³¼ë¥¼ ì¢…í•©í•©ë‹ˆë‹¤.  

ì¦‰, Ensembleì€ **ëª¨ë¸ì˜ ë‹¤ì–‘ì„±ì„ í™œìš©í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë°©ë²•**ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## MoE vs Ensemble: í•µì‹¬ ì°¨ì´

| êµ¬ë¶„ | Ensemble | MoE |
|------|----------|-----|
| **êµ¬ì¡°** | ì—¬ëŸ¬ ëª¨ë¸ + ë‹¨ìˆœ í‰ê· /íˆ¬í‘œ | ì—¬ëŸ¬ Expert + Gating Network |
| **Expert ì‚¬ìš© ë°©ì‹** | ëª¨ë“  ì…ë ¥ì´ ëª¨ë“  ëª¨ë¸ì„ ê±°ì¹¨ | ì…ë ¥ë§ˆë‹¤ ì¼ë¶€ Expertë§Œ ì„ íƒ |
| **ê²Œì´íŠ¸(ì¡°ê±´ë¶€ ì„ íƒ)** | ì—†ìŒ | ìˆìŒ |
| **ì—°ì‚° íš¨ìœ¨** | $M$ê°œ ëª¨ë¸ ì „ë¶€ ê³„ì‚° â†’ ë¹„ìš© â†‘ | ì„ íƒëœ $k$ê°œ Expertë§Œ ê³„ì‚° â†’ íš¨ìœ¨ â†‘ |
| **ëª©í‘œ** | ëª¨ë¸ ì˜ˆì¸¡ ì•ˆì •ì„±Â·ì¼ë°˜í™” | ë°ì´í„° ë¶„í¬ë³„ **ì „ë¬¸í™”ëœ ì²˜ë¦¬** |
| **Expert í˜•íƒœ** | ëŒ€ì²´ë¡œ ìœ ì‚¬í•œ êµ¬ì¡° | ì‹ ê²½ë§, íšŒê·€, íŠ¸ë¦¬, ê·œì¹™ ë“± ë¬´ì—‡ì´ë“  ê°€ëŠ¥ |
| **í•´ì„ ê°€ëŠ¥ì„±** | ì–´ë–¤ ëª¨ë¸ì´ ê¸°ì—¬í–ˆëŠ”ì§€ ë¶ˆë¶„ëª… | ê²Œì´íŠ¸ê°€ ì„ íƒí•œ Expert í™•ì¸ ê°€ëŠ¥ |

---

## ì§ê´€ì  ì´í•´

- **Ensemble**: "ì—¬ëŸ¬ ëª¨ë¸ì˜ ì§‘ë‹¨ì§€ì„±"  
- **MoE**: "ê²Œì´íŠ¸ê°€ ì „ë¬¸ê°€ì—ê²Œ ì¼ì„ ë‚˜ëˆ„ì–´ì£¼ëŠ” ì¡°ì§"  

ì¦‰, Ensembleì€ **ë¬´ì¡°ê±´ ë‹¤ ê°™ì´ ì°¸ì—¬**,  
MoEëŠ” **ìƒí™©ë³„ë¡œ ì•Œë§ì€ ì „ë¬¸ê°€ë§Œ ì°¸ì—¬**í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

---

## ê°„ë‹¨í•œ PyTorch ì˜ˆì œ (MLP Expert Ver.)

ì•„ë˜ëŠ” ì…ë ¥ì— ë”°ë¼ Expertë¥¼ ì„ íƒí•˜ëŠ” ê°„ë‹¨í•œ MoE êµ¬í˜„ì…ë‹ˆë‹¤.

{% include code-header.html %}
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Expert(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim)
        )
    def forward(self, x):
        return self.net(x)

class MoE(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, n_experts=4, k=1):
        super().__init__()
        self.experts = nn.ModuleList([Expert(in_dim, hidden_dim, out_dim) for _ in range(n_experts)])
        self.gate = nn.Linear(in_dim, n_experts)
        self.k = k

    def forward(self, x):
        gate_logits = self.gate(x)                  # (batch, n_experts)
        topk_val, topk_idx = torch.topk(gate_logits, self.k, dim=-1)
        weights = F.softmax(topk_val, dim=-1)

        out = torch.zeros(x.size(0), self.experts[0].net[-1].out_features)
        for i in range(self.k):
            for b in range(x.size(0)):
                out[b] += weights[b, i] * self.experts[topk_idx[b, i]](x[b].unsqueeze(0))
        return out
```
---

## Conclusion
  - Ensembleì€ ë‹¨ìˆœíˆ ì—¬ëŸ¬ ëª¨ë¸ì„ ë™ì‹œì— í™œìš©í•˜ëŠ” ê¸°ë²•.
  - MoEëŠ” ê²Œì´íŒ… ë„¤íŠ¸ì›Œí¬ë¥¼ ë‘ì–´ ì…ë ¥ë³„ë¡œ Expertë¥¼ ë‹¤ë¥´ê²Œ ì„ íƒí•˜ëŠ” ê¸°ë²•.
  - ExpertëŠ” ê¼­ ì‹ ê²½ë§ì¼ í•„ìš”ê°€ ì—†ìœ¼ë©°, í†µê³„ ëª¨ë¸ì´ë‚˜ ê·œì¹™ ê¸°ë°˜ ëª¨ë“ˆë„ Expertê°€ ë  ìˆ˜ ìˆë‹¤.

ë”°ë¼ì„œ MoEëŠ” ë‹¨ìˆœ Ensembleë³´ë‹¤ íš¨ìœ¨ì ì´ê³ ,
**ë°ì´í„° ë¶„í¬ê°€ ë‹¤ì–‘**í•˜ê±°ë‚˜ **ì¡°ê±´ë¶€ íŠ¹ì„±ì´ ëšœë ·**í•œ ë¬¸ì œì—ì„œ íŠ¹íˆ ê°•ë ¥í•©ë‹ˆë‹¤.

---


## Reference
 - [Hierarchical mixtures of experts and the EM algorithm](https://ieeexplore.ieee.org/document/381768)
 - [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)
 - [Hugging Face - Mixture of Experts Explained](https://huggingface.co/blog/moe)
 - [A Visual Guide to Mixture of Experts (MoE)
](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)