---
layout: single
title: "[Paper Review] Generalized Variational Inference: Three arguments for deriving new posteriors"
date: 2025-12-06
permalink: /generalized-variational-inference/
categories:
  - Statistics
  - Bayesian
  - Machine Learning
tags:
  - Generalized Variational Inference
  - Bayesian Deep Learning

toc: true
toc_sticky: true
comments: true
---

> 이 포스팅은 Knoblauch, Jewson, Damoulas (2019) 의  
> [“Generalized Variational Inference: Three arguments for deriving new posteriors”](https://arxiv.org/pdf/1904.02063) 를 읽고 정리한 글입니다. 

---

## Introduction

### Problem and Proposed Solution

베이지안 추론은 다음과 같은 형태의 posterior 를 사용합니다.

$$
q_B^*(\theta \mid x_{1:n})
\propto \pi(\theta)\; p_n(x_{1:n} \mid \theta)
$$

여기서 $\pi$ 는 prior, $p_n$ 은 likelihood 입니다.

논문 저자들은 이 posterior 를 실제로 사용하기 위해서는 암묵적으로 **세 가지 가정**이 수반된다고 강조합니다.

1. **Well-specified prior**  
   - 사전 지식을 잘 반영하는 prior $\pi(\theta)$ 를 알고 있고, 그걸 수학적으로 잘 표기할 수 있다고 가정합니다.

2. **Well-specified likelihood model**  
   - 실제 데이터의 분포가 $p_n(x_{1:n}\mid \theta)$ 꼴이라고 믿습니다. (모델이 참이라고 가정)

3. **Infinite computing power**  
   - 일반적으로 intractable 한 posterior $q_B^*(\theta)$ 를 **정확하게** 다룰 수 있다고 봅니다.

</br>

작거나 전통적인 통계 task 에서는 이 세 가지 가정이 그럭저럭 성립할 수 있지만,  
데이터 및 모델의 규모가 커질수록 현실과의 간극이 커집니다.

- prior 는 사실상 “fake” 인 경우가 많습니다.  
- likelihood 는 misspecification, heavy tails, contamination 이 흔합니다.  
- posterior 는 너무 커서 MCMC, standard VI 로도 다루기 힘든 경우가 많습니다.

</br>

따라서 저자들은 아예 **Bayes rule 을 최적화 문제로 재해석**하고,  
그 틀을 일반화하여 “새로운 posterior” 를 설계하자고 제안합니다.

그 결과물이 바로

- **Rule of Three (RoT)** : 3개의 argument 로 정의되는 generalized posterior  
- **GVI (Generalized Variational Inference)** : RoT 의 tractable 한 특수 케이스  

입니다.

---

## Method 1

### Bayes의 Multiplicative Form

먼저 generalized Bayesian posterior 를 다음과 같이 씁니다.

$$
q_B^*(\theta)
\propto
\pi(\theta)\prod_{i=1}^n \exp\{-\ell(\theta, x_i)\}.
$$

- **Standard Bayes** 인 경우  
  $\ell(\theta, x_i) = -\log p(x_i \mid \theta)$ 입니다.
- **Generalized Bayes** 인 경우  
  $\ell$ 은 임의의 loss function 입니다. (예: median 추론이면 $|\theta - x_i|$)

이 multiplicative form 은 **loss 를 quasi-likelihood** 로 보는 관점에서도 해석할 수 있습니다.  
(예: Bissiri et al. 의 generalized Bayes)

---

### Bayes Posterior는 무한 차원에서의 최적화 문제

[Csiszár (1975)](https://projecteuclid.org/journals/annals-of-probability/volume-3/issue-1/I-Divergence-Geometry-of-Probability-Distributions-and-Minimization-Problems/10.1214/aop/1176996454.full) 와  
[Donsker & Varadhan (1975)](https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160280102) 에 따르면,  
Bayes posterior 는 다음과 같은 **무한 차원 최적화 문제의 해**로 쓸 수 있습니다.

$$
q_B^*
=
\arg\min_{q \in \mathcal P(\Theta)}
\bigg\{
\mathbb{E}_q\bigg[\sum_{i=1}^n \ell(\theta, x_i)\bigg]
+ \mathrm{KL}(q \,\|\, \pi)
\bigg\}
$$

- $\mathcal P(\Theta)$ : $\Theta$ 위의 **모든 확률분포들의 집합**  
- 첫 항 : 데이터에 대한 **loss 의 기댓값**  
- 둘째 항 : prior 와의 **KLD regularization**  

즉,

> **Bayes posterior 는  
> “loss + KLD” 를 최소화하는 무한 차원 최적화 문제의 해입니다.**

이것이 논문에서 계속 끌고 가는 핵심 isomorphism 입니다.

---

### Optimality of Standard VI

Exact posterior 는 계산 cost 가 크기 때문에,  
posterior 후보를 어떤 variational family  
$\mathcal Q = \{ q(\theta \mid \kappa) : \kappa \in K \}$ 로 제한합니다.

$$
\text{original: } \min_{q \in \mathcal P(\Theta)} \;\;\longrightarrow\;\;
\text{constrained: } \min_{q \in \mathcal Q}
$$

그러면 최적화 문제는 다음과 같이 바뀝니다.

$$
q_{\mathrm{VI}}^*
=
\arg\min_{q\in \mathcal Q}
\bigg\{
\mathbb{E}_q\bigg[\sum_{i=1}^n \ell(\theta, x_i)\bigg]
+ \mathrm{KL}(q \,\|\, \pi)
\bigg\}
$$

이 식은 우리가 알고 있는 **standard VI 의 objective (ELBO 최대화와 동치)** 와 동일합니다.

> **즉, standard VI 는  
> variational family $\mathcal Q$ 가 고정된 상황에서  
> Bayes optimization 문제의 “제한된 최적해”입니다.**

같은 $\mathcal Q$ 를 사용하면서 **다른 divergence** 를 최적화하는 VI 들(α-VI, Rényi-VI 등)은  
이 base optimization 문제를 그대로 계승하지 않기 때문에,  
저자들은 이를 “Bayesian optimality 관점에서 suboptimal 하다”라고 봅니다.

---

## Method 2: Rule of Three (RoT)

### RoT: 세 개의 argument 로 정의되는 generalized posterior

앞의 isomorphism 을 보고 나면 자연스럽게 다음과 같은 생각을 할 수 있습니다.

> “그렇다면 **loss, divergence, feasible set** 을 바꾸면  
> 새로운 posterior 들을 통일된 방식으로 정의할 수 있지 않을까?”

이 생각을 공식화한 것이 **Rule of Three (RoT)** 입니다.

수식으로 쓰면 다음과 같습니다.

$$
q^*(\theta)
=
\arg\min_{q\in \Pi}
\bigg\{
\mathbb{E}_q\bigg[\sum_{i=1}^n \ell(\theta, x_i)\bigg]
+ D(q \,\|\, \pi)
\bigg\}
\;\;\overset{\text{def}}{=}\;\;
P(\ell, D, \Pi)
$$

여기서 세 가지 argument 는 다음과 같습니다.

1. **Loss $\ell(\theta, x)$**  
   - 반드시 $-\log p(x\mid\theta)$ 일 필요는 없습니다.  
   - 예: median 추론이면 absolute error $|\theta - x|$, robust loss, scoring rule 등입니다.

2. **Divergence $D(q \,\|\, \pi)$**  
   - prior 와 posterior 사이의 distance 입니다.  
   - Standard Bayes 에서는 $D = \mathrm{KL}$ 입니다.  
   - 대신 Rényi divergence, β-divergence 등 다른 divergence 도 사용할 수 있습니다.

3. **Feasible set $\Pi \subset \mathcal P(\Theta)$**  
   - posterior 후보 분포들의 집합입니다.  
   - $\Pi = \mathcal P(\Theta)$ 이면 “full” generalized Bayes 입니다.  
   - $\Pi = \mathcal Q$ 이면 variational inference 가 됩니다.

이 RoT 안에서 여러 방법들을 special case 로 볼 수 있습니다.

- **Standard Bayes**
  - $\ell = -\log \text{Likelihood}$  
  - $D = \mathrm{KL}$  
  - $\Pi = \mathcal P(\Theta)$  

- **Standard VI**
  - $\ell = -\log \text{Likelihood}$  
  - $D = \mathrm{KL}$  
  - $\Pi = \mathcal Q$ (variational family)  

- **Generalized Bayes**
  - $\ell$ 또는 $D$ 를 수정하고, $\Pi$ 는 전체 공간으로 유지합니다.

---

### Axioms

RoT 가 만족해야 할 **세 가지 공리(Axioms)** 가 있습니다 (논문 4.1절).

대략적으로 정리하면 다음과 같습니다.

1. **Coherence / Consistency**  
   - loss 와 divergence 를 어떻게 조합해야  
     “일관된 belief 업데이트” 가 되는지에 대한 조건입니다.

2. **Modular decomposition**  
   - loss 는 **goodness of fit** (어떤 $\theta$ 가 좋은지),  
   - divergence 는 **belief regularization** (prior 대비 얼마나 멀어질 수 있는지)  
     를 담당해야 합니다.  
   - divergence 를 바꿀 때, loss 의 의미를 훼손하면 안 됩니다.

3. **Invariance / Reparameterization 등 기술적 조건**  
   - reparametrization 에 대해 잘 작동하는지,  
   - 적절한 regularity / measurability 등을 만족하는지에 대한 조건입니다.

저자들은 **이 공리들을 만족하는 generalized posterior 는 RoT 형태로 표현될 수 있다**고 주장합니다.  
즉, RoT 는 “합리적인 generalized Bayesian 업데이트” 들을 하나의 형태로 묶는 프레임워크라고 볼 수 있습니다.


<figure style="text-align: center;">
  <img src="/assets/img/gvi/image.png" alt="RoT" width = 1000/>
</figure>

---


## Method 3: Generalized Variational Inference (GVI)

이제 RoT 는 정의되었지만, 여전히 $\Pi = \mathcal P(\Theta)$ 로 두면 계산 cost 가 매우 크게 됩니다.

그래서 tractable 한 **special case** 로서 **GVI** 를 정의합니다.

$$
\Pi = \mathcal Q = \{q(\theta \mid \kappa) : \kappa \in K\}
\subset \mathcal P(\Theta)
$$

를 variational family 로 잡아서 RoT 를 푸는 것을 **GVI** 라고 부릅니다.

즉, GVI 의 objective 는 다음과 같습니다.

$$
q_{\mathrm{GVI}}^*
=
\argmin_{q \in \mathcal Q}
\bigg\{
\mathbb{E}_q\bigg[\sum_{i=1}^n \ell(\theta, x_i)\bigg]
+ D(q \,\|\, \pi)
\bigg\}
$$

- $\ell$ : 원하는 loss (NLL, robust loss, scoring rule, pseudo-likelihood 등)  
- $D$ : 원하는 divergence (KL, Rényi, β-divergence 등)  
- $\mathcal Q$ : mean-field Gaussian, normalizing flow, BNN 등 variational family  

GVI 는 RoT 의 특수 케이스이기 때문에 **Axiom 1, 2, 3 을 모두 만족**합니다.

이로부터 다음과 같은 해석이 가능합니다.

- Loss $\ell$ 은 **어떤 $\theta$ 가 좋은가**를 결정하는 역할,  
- Divergence $D$ 는 **prior 대비 posterior 가 얼마나 멀어질 수 있는가**를 규제하는 역할을 합니다.

따라서 두 요소를 **분리해서 조절할 수 있습니다.**

- $\ell$ 을 바꾸면 “best parameter” 의 선택 기준이 바뀌고,  
- $D$ 를 바꾸면 **uncertainty quantification** 의 크기/모양만 바뀌는 식입니다.

---

### GVI vs DVI (Discrepancy VI)

최근 VI 문헌에서는, posterior 와의 KL 대신 다른 divergence (또는 discrepancy) 를 쓰는 방식들이 많이 제안됩니다.

- Rényi-α divergence 기반 VI  
- χ²-divergence 기반 VI  
- Wasserstein distance 기반 VI  
- 기타 f-divergence 기반 VI 등

논문에서는 이런 계열을 통칭해 **Discrepancy Variational Inference (DVI)** 라고 부르며,  
RoT/GVI 관점에서 다음과 같이 비판합니다.

1. **같은 $\mathcal Q$ 를 쓰면서 posterior 와의 divergence 를 바꾸는 것은 Bayesian 의미에서 suboptimal**입니다.  
   - Bayes-as-optimization 관점에서 base 문제는 이미  
     $$
     \min_q \big\{\mathbb E_q[\ell] + \mathrm{KL}(q\|\pi)\big\}
     $$
     로 정해져 있습니다.
   - variational family $\mathcal Q$ 로 제한하면,  
     이 문제의 최적해는 **KL 기반 VI 하나뿐**입니다.
   - posterior 와의 다른 divergence 를 직접 줄이는 DVI 는  
     이 base optimization 의 “제한된 해” 가 아니므로,  
     이러한 의미에서 **Bayesian optimality 를 잃는다**고 주장합니다.

2. **Modularity 가 깨집니다.**  
   - GVI (RoT) 에서는
     - $\ell$ 이 “어떤 $\theta$ 가 좋은가” 를  
     - $D$ 가 “prior 대비 얼마나 멀어질 수 있는가 / dispersion” 을  
       담당하도록 설계되어 있습니다.
   - 따라서 divergence 를 바꾸더라도, 이상적으로 **argmin 위치(최선의 $\theta$)** 는  
     loss 가 결정하고 유지되며, $D$ 는 “그 주변에 mass 를 얼마나/어떻게 둘지” 만 조정해야 합니다.
   - 반면 DVI 는 posterior 와의 divergence 를 직접 최적화하기 때문에,  
     divergence 를 바꾸는 순간 **최적점 위치 자체까지 같이 움직여 버릴 수 있습니다.**  
     → loss 역할과 regularization 역할이 뒤엉기게 됩니다.

3. **실제 예시: Bimodal mixture 에서의 비직관적 posterior**  
   - 논문은 simple bimodal Gaussian mixture 예시에서
     - standard VI (KL, mean-field) 와
     - 여러 DVI (다른 discrepancy) 를 비교합니다.
   - 일부 discrepancy 는
     - 두 모드 사이의 어중간한 곳에 mass 를 두거나,
     - 한 모드만 과도하게 살리는 등,
       직관적인 “좋은 parameter” 와는 동떨어진 posterior 를 만듭니다.
   - 저자들의 메시지는 다음과 같습니다.  
     > 단순히 divergence 를 바꾸는 것만으로  
     > 항상 “좋은 approximate posterior” 를 얻을 수 있다고 믿는 것은 위험하다.

이에 반해 GVI 는 RoT/axioms 를 따르기 때문에

- $\ell$ 과 $D$ 의 역할이 명확히 분리되고 (modular),  
- $D$ 를 바꾸더라도 “최선의 $\theta$” 는 동일하게 유지한 채,  
- **불확실성의 크기와 모양만 조정**하는 것이 가능하다고 주장합니다.

---

### Black-box GVI (BBGVI)

실제 계산 관점에서 보면, GVI objective 는

$$
\mathcal J(q)
=
\mathbb E_q\bigg[\sum_{i=1}^n \ell(\theta, x_i)\bigg]
+ D(q \,\|\, \pi)
$$

이며, 우리는 $q(\theta)$ 를 어떤 파라미터 $\kappa$ 를 가진 분포 $q_\kappa(\theta)$ 로 표현합니다.

대부분의 경우 위 기대값과 regularization 항은 **analytic 하지 않기 때문에**,  
논문에서는 **Black-box GVI (BBGVI)** 라는 이름으로 일반적인 최적화 스킴을 제안합니다.

아이디어는 standard black-box VI 와 거의 동일합니다.

1. **Variational family 파라미터화**  
   - $q_\kappa(\theta)$ 의 예시:
     - Mean-field Gaussian: $\kappa = (\mu, \log \sigma)$  
     - Normalizing flow: base Gaussian + invertible transform  
     - BNN: 네트워크 weight 에 대한 factorized Gaussian 등  

2. **Reparameterization / 샘플링**  
   - 고정된 noise $\epsilon \sim p(\epsilon)$ 를 샘플링하고,  
   - $\theta = g(\epsilon; \kappa)$ 로 변환하여 $\theta \sim q_\kappa$ 를 구현합니다.

3. **Monte Carlo 로 objective 및 gradient 근사**  
   - 샘플 $\{\theta^{(s)}\}$ 에 대해
     $$
     \widehat{\mathcal J}(\kappa)
     =
     \frac{1}{S}\sum_{s=1}^S
     \bigg[
       \sum_{i=1}^n \ell(\theta^{(s)}, x_i)
     \bigg]
     + \widehat{D}(q_\kappa \,\|\, \pi)
     $$
   - $D$ 역시 analytic form 이 없으면 Monte Carlo / score function / reparam gradient 등으로 근사합니다.
   - reparameterization trick 덕분에  
     $\nabla_\kappa \widehat{\mathcal J}(\kappa)$ 는 backprop 으로 계산할 수 있습니다.

4. **Stochastic optimization**  
   - 미니배치 $x_{1:m}$ 을 사용하면 SVI 스타일로 확장할 수 있습니다.  
   - Adam / SGD 등을 사용해  
     $\kappa \leftarrow \kappa - \eta \nabla_\kappa \widehat{\mathcal J}(\kappa)$ 로 업데이트합니다.

Standard VI (ELBO, NLL + KL) 도 이 틀의 특수 케이스이므로,  
기존 black-box VI 코드가 있다면 **loss $\ell$ 과 divergence $D$ 만 바꾸어 GVI 실험**을 수행할 수 있다는 점을 강조합니다.

---

### Applications: BNN, Deep GP 에서의 GVI

논문 후반부에서는 GVI 를 실제 모델에 적용한 실험을 보여줍니다. (간단 요약)

1. **Bayesian Neural Networks (BNN)**  
   - Standard VI (NLL + KL, mean-field) 와  
     GVI (robust loss / tempered divergence 등) 를 비교합니다.  
   - heavy-tailed noise, outlier 가 많은 상황에서
     - standard VI 는 overconfident / miscalibrated posterior 를 생성하는 반면,  
     - GVI 는 분산을 더 적절히 키우고, 예측 calibration 이 개선되는 모습을 보입니다.

2. **Deep Gaussian Processes (Deep GP)**  
   - generalized loss + divergence 를 넣은 GVI 를 적용합니다.  
   - Noise / model misspecification 세팅에서  
     GVI 가 standard VI 보다 더 robust 한 predictive performance 를 보인다고 보고합니다.

이 실험들은 “loss 와 divergence 라는 두 개의 다이얼을 돌려서,  
Bayesian 추론의 **robustness / calibration / misspecification 대응력**을 조절할 수 있다”는 주장의 정성적 근거로 등장합니다.

---

## Discussion & Takeaways

마지막으로, 이 논문을 정리하면 다음과 같습니다.

1. **Bayes 를 “최적화 문제”로 이해하자.**  
   - Bayes posterior 는
     $$
     \min_q \big\{ \mathbb E_q[\text{loss}] + \mathrm{KL}(q \,\|\, \pi) \big\}
     $$
     라는 무한 차원 최적화 문제의 해입니다.  
   - 이 관점은 VI, EM, generalized Bayes, PVI 등을 한 프레임에서 비교·확장하는 데 매우 유용합니다.

2. **Standard VI(KL + ELBO) 는, 주어진 family 안에서는 “정석”이다.**  
   - Variational family $\mathcal Q$ 가 고정되어 있을 때,  
     KL 기반 VI 는 Bayes-as-optimization 을 $\mathcal Q$ 로 제한한 문제의 최적해입니다.  
   - 같은 $\mathcal Q$ 에서 다른 divergence 기반 VI(DVI)를 사용할 경우,  
     Bayesian optimality 관점에서 “왜 그것이 자연스러운 선택인지”를 추가로 설명해야 합니다.

3. **RoT: 세 개의 다이얼로 posterior 를 디자인하는 프레임워크**  
   - Loss $\ell$ : 어떤 파라미터가 좋은지 (fit / scoring rule)  
   - Divergence $D$ : prior 대비 얼마나 멀어질 수 있는지, 불확실성의 크기/모양  
   - Feasible set $\Pi$ : 계산·표현 측면에서의 제약 (full, variational, flow, …)  

   → 이 세 가지를 동시에 튜닝하면서 “새로운 posterior” 를 설계하는 통일된 언어를 제공합니다.

4. **GVI: RoT 를 실제로 쓰기 위한 variational 버전**  
   - $\Pi = \mathcal Q$ 로 제한하여 tractable 하게 만들고,  
   - Black-box 최적화(SVI, reparam, MC gradient) 를 사용해 구현할 수 있습니다.  
   - RoT 의 공리를 계승하므로, loss 와 regularizer 의 역할이 modular 하게 유지된다는 장점이 있습니다.

5. **PVI / scoring-rule 기반 VI 와의 연결**  
   - 우리가 관심 있는 **Predictive Variational Inference (PVI)** 는  
     RoT/GVI 에서 보면 “loss $\ell$ 을 log-score 대신 다른 proper scoring rule 로 바꾸는 조합”으로 자연스럽게 해석할 수 있습니다.  
   - 즉, 이 논문은 PVI 를  
     “Bayes-as-optimization 의 한 generalization” 으로 어디에 위치시킬지 설명해 주는 이론적 배경이 됩니다.

