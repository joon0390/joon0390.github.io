---
layout: single
title: "Bayesian ICA"
date: 2025-10-02
permalink: /bica/
categories:
  - Statistics
  - Bayesian
tags:
  - ICA
  - Variational Inference
  - Latent Variable Model
  
toc: true
toc_sticky: true
comments: true
---

> ì´ í¬ìŠ¤íŒ…ì€ **Bayesian Independent Component Analysis (Bayesian ICA)** ì— ëŒ€í•´ ê³µë¶€í•˜ë©° ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.  
> ì „í†µì ì¸ ICAì˜ ë…ë¦½ì„± ê°€ì •ì„ ë² ì´ì§€ì•ˆ í”„ë ˆì„ì›Œí¬ë¡œ í™•ì¥í•˜ì—¬, ë¶ˆí™•ì‹¤ì„± ì¶”ì •ê³¼ ìë™ ëª¨ë¸ ì„ íƒ(ARD)ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì ‘ê·¼ì…ë‹ˆë‹¤.  

---

## Idea of ICA

ICAì˜ ê¸°ë³¸ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

$$
\boldsymbol{x} = \boldsymbol{A}\boldsymbol{s} + \boldsymbol{\epsilon}
$$

- $\boldsymbol{x} \in \mathbb{R}^{D}$ : ê´€ì¸¡ ë°ì´í„° (observed data)  
- $\boldsymbol{s} \in \mathbb{R}^{K}$ : **ë…ë¦½ëœ ì ì¬ ë³€ìˆ˜ (independent sources)**  
- $\boldsymbol{A} \in \mathbb{R}^{D \times K}$ : Mixing Matrix  
- $\boldsymbol{\epsilon}$ : ë…¸ì´ì¦ˆ (ë³´í†µ $\mathcal{N}(0, \sigma^2I)$)

ICAì˜ ëª©í‘œëŠ” $A$ì™€ $\boldsymbol{s}$ë¥¼ ì¶”ì •í•˜ëŠ” ê²ƒì¸ë°, ì´ë•Œ $\boldsymbol{s}$ì˜ ê° ì„±ë¶„ì´ **í†µê³„ì ìœ¼ë¡œ ë…ë¦½ì (independent)** ì´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.

ì¦‰, ê´€ì¸¡ëœ ë°ì´í„°ëŠ” ì„œë¡œ ë…ë¦½ì ì¸ ì ì¬ ë³€ìˆ˜ë“¤ì˜ **ì„ í˜• í˜¼í•©(linear mixture)** ì´ë¼ëŠ” ê²ƒì´ í•µì‹¬ ê°€ì •ì…ë‹ˆë‹¤.

$$
p(\boldsymbol{s}) = \prod_{k=1}^K p(s_k)
$$

> ì „í†µì ì¸ ICAëŠ” Maximum Likelihood ê¸°ë°˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì  ì¶”ì •(point estimate)í•˜ì§€ë§Œ,  
> **Bayesian ICA**ëŠ” íŒŒë¼ë¯¸í„° ì „ì²´ë¥¼ í™•ë¥ ë³€ìˆ˜ë¡œ ë‘ì–´ **ì‚¬í›„ë¶„í¬(posterior distribution)** ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.

---

## Bayesian ICA Model

Bayesian ICAì˜ ì „ì²´ í™•ë¥  ëª¨í˜•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

$$
p(\boldsymbol{x}, \boldsymbol{s}, \boldsymbol{A}, \sigma^2)
= p(\boldsymbol{x} | \boldsymbol{A}, \boldsymbol{s}, \sigma^2)\,
  p(\boldsymbol{s})\, p(\boldsymbol{A})\, p(\sigma^2)
$$

- **Likelihood**
  $$
  p(\boldsymbol{x} | \boldsymbol{A}, \boldsymbol{s}, \sigma^2)
  = \mathcal{N}(\boldsymbol{x} | \boldsymbol{A}\boldsymbol{s}, \sigma^2 I)
  $$

- **Prior on sources** (ë…ë¦½ ë¹„ê°€ìš°ì‹œì•ˆ ë¶„í¬; e.g., Laplace Prior)
  $$
  p(s_k) = \frac{1}{Z}\exp(-|s_k|)
  $$
  ì—¬ê¸°ì„œ $Z$ëŠ” ì •ê·œí™” ìƒìˆ˜ì´ë©°, ì´ í˜•íƒœëŠ” Laplace(0,1) ë¶„í¬ì— í•´ë‹¹í•©ë‹ˆë‹¤.  
  ì´ëŠ” ëŒ€ë¶€ë¶„ì˜ $s_k$ê°€ 0 ê·¼ì²˜(í¬ì†Œì„±)ì´ê³ , ê°€ë” í° ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ  
  **ë¹„ê°€ìš°ì‹œì•ˆì„±(non-Gaussianity)** ê³¼ **sparsity**ë¥¼ ë™ì‹œì— ë¶€ì—¬í•©ë‹ˆë‹¤.  

  <br>

  > ë§Œì•½ $p(s_k)$ê°€ Gaussianì´ë¼ë©´ ICAëŠ” ì‹ë³„ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.  
  > ì¦‰, ë…ë¦½ ì›ì²œì„ êµ¬ë¶„í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, ë°˜ë“œì‹œ ë¹„ê°€ìš°ì‹œì•ˆ priorê°€ í•„ìš”í•©ë‹ˆë‹¤.

- **Prior on Mixing Matrix**
  $$
  p(\boldsymbol{A}) = \prod_{i,j}\mathcal{N}(A_{ij} | 0, \tau^2)
  $$

- **Noise Prior**
  $$
  p(\sigma^2) \sim \text{Inverse-Gamma}(\alpha, \beta)
  $$

---

## Inference

ICAëŠ” ì ì¬ ë³€ìˆ˜ $\boldsymbol{s}$ê°€ ë¹„ê°€ìš°ì‹œì•ˆ(non-Gaussian)ì„ ë”°ë¥´ê¸° ë•Œë¬¸ì—,  
Posterior $p(\boldsymbol{s}, \boldsymbol{A} | \boldsymbol{x})$ëŠ” ë‹«íŒ í˜•íƒœë¡œ ê³„ì‚°ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  
ë”°ë¼ì„œ **ê·¼ì‚¬ì¶”ë¡ (Approximate Inference)** ê¸°ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.  

---

### (1) Variational Bayesian ICA (VB-ICA)

#### Factorized Posterior ê°€ì •
$$
q(\boldsymbol{s}, \boldsymbol{A}) = q(\boldsymbol{A}) \prod_n q(\boldsymbol{s}_n)
$$

#### ELBO (Evidence Lower Bound)
$$
\mathcal{L} = 
\mathbb{E}_q[\log p(\boldsymbol{X}, \boldsymbol{S}, \boldsymbol{A}) - \log q(\boldsymbol{S}, \boldsymbol{A})]
$$

ì´ í•˜í•œ(ELBO)ì„ ìµœëŒ€í™”í•˜ë©´, ì‹¤ì œ ë¡œê·¸ ì‚¬í›„ê°€ëŠ¥ë„(log-evidence)ë¥¼ ê·¼ì‚¬í•˜ê²Œ ë©ë‹ˆë‹¤.

#### VB-EM í˜•íƒœì˜ ì—…ë°ì´íŠ¸
- **E-step:** $q(\boldsymbol{s}_n)$ ì—…ë°ì´íŠ¸  
  $$
  q(\boldsymbol{s}_n) \propto 
  \exp\left(
  \mathbb{E}_{q(\boldsymbol{A})}[\log p(\boldsymbol{x}_n | \boldsymbol{A}, \boldsymbol{s}_n)]+ \log p(\boldsymbol{s}_n) \right)$$

- **M-step:** $q(\boldsymbol{A})$, $\sigma^2$, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

#### ìˆ˜ë ´ í›„
Posterior í‰ê· ì„ ì¶”ì •ì¹˜ë¡œ ì‚¬ìš©:
$$
\hat{\boldsymbol{S}} = \mathbb{E}_q[\boldsymbol{S}], \quad
\hat{\boldsymbol{A}} = \mathbb{E}_q[\boldsymbol{A}]
$$

---

### (2) MCMC ê¸°ë°˜ Bayesian ICA

- Gibbs sampling í˜¹ì€ Hamiltonian Monte Carlo(HMC)ë¥¼ ì´ìš©í•´  
  $p(\boldsymbol{A}, \boldsymbol{S} | \boldsymbol{X})$ì—ì„œ ì§ì ‘ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
- ì´ëŠ” Posterior ë¶ˆí™•ì‹¤ì„±ì„ ì™„ì „í•˜ê²Œ ë°˜ì˜ ê°€ëŠ¥í•©ë‹ˆë‹¤. 
- í•˜ì§€ë§Œ ê³„ì‚°ëŸ‰ì´ ë§¤ìš° í¬ë‹¤ëŠ” ë‹¨ì ë„ ì¡´ì¬í•©ë‹ˆë‹¤.

---

##  Variational Bayesian ICA Summary

| ë‹¨ê³„ | ì„¤ëª… |
|------|------|
| 1ï¸âƒ£ ì´ˆê¸°í™” | $q(\boldsymbol{A}) = \mathcal{N}(\boldsymbol{A}_0, \Sigma_A)$ |
| 2ï¸âƒ£ ë°˜ë³µ | ê° ë°ì´í„° $x_n$ì— ëŒ€í•´ $q(\boldsymbol{s}_n)$ ì—…ë°ì´íŠ¸ |
| 3ï¸âƒ£ ê°±ì‹  | $\boldsymbol{A}$ì˜ Posterior ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ |
| 4ï¸âƒ£ ìˆ˜ë ´ | Posterior Meanì„ ì¶”ì •ê°’ìœ¼ë¡œ ì‚¬ìš© |

---

## ğŸ“š References

- Attias, H. (1999). [*Independent Factor Analysis*. Neural Computation.](https://psycnet.apa.org/record/1999-13540-001)
- [Wikipedia ICA](https://en.wikipedia.org/wiki/Independent_component_analysis)
- Alaa, T. (2020). [Independent component analysis: An introduction](https://www.emerald.com/aci/article/17/2/222/6032/Independent-component-analysis-An-introduction)