---
layout: single
title: "Probabilistic PCA"
date: 2025-09-30
permalink: /ppca/
categories:
  - Statistics
  - Bayesian
tags:
  - Dimensionality Reduction
  - PPCA
  - PCA

toc: true
toc_sticky: true
comments: true
---

> 이 포스팅은 [Probabilistic Principal Component Analysis (PPCA)](https://www.cs.columbia.edu/~blei/seminar/2020-representation/readings/TippingBishop1999.pdf)에 대해 공부하며 정리한 글입니다.  
> Tipping & Bishop (1999)의 고전 논문을 기반으로 수학적 정의, 추론 과정, 장단점을 설명하겠습니다.

## Background
- **PCA**는 데이터의 공분산 행렬을 고유분해하여 저차원 공간으로 투영하는 선형 차원축소 기법입니다.  
- 하지만 PCA는 **선형 대수적 접근**일 뿐, 확률 모델로 정의되지 않습니다.  
- 따라서
  - 불확실성을 추정 할 수 없고
  - 결측치(missing data) 처리가 불가능하며
  - 다른 확률적 모델과의 통합이 어렵다는 단점이 존재합니다. 
> 이를 해결하기 위해 PCA를 **확률적 생성 모델**로 재해석한 것이 PPCA입니다.

## Model Definition
PPCA는 잠재변수 모형(latent variable model)로 정의됩니다. 즉, 데이터는 숨겨진 저차원 변수로부터 생성된다고 가정합니다.

$$
x = W z + \mu + \epsilon
$$

- $x \in \mathbb{R}^d$: 관측 벡터  
- $z \in \mathbb{R}^q$: 잠재 벡터 ($q \ll d$)  
- $W \in \mathbb{R}^{d \times q}$: 로딩 행렬  
- $\mu \in \mathbb{R}^d$: 평균 벡터  
- $\epsilon \sim \mathcal{N}(0, \sigma^2 I_d)$: 등방성 가우시안 노이즈  
- $z \sim \mathcal{N}(0, I_q)$: 표준 정규 분포  

따라서 주변 분포는 다음과 같습니다.

$$
x \sim \mathcal{N}(\mu, W W^\top + \sigma^2 I_d)
$$

즉 PCA가 찾는 저차원 공간을 **확률적 생성 모델**로 정의한 것이 PPCA입니다.

## Parameter Estimation
목표는 $W, \mu, \sigma^2$를 학습하는 것입니다. 데이터 로그가능도를 최대화하여 추정합니다.

### Two Approaches
1. **Closed-form MLE**  
   표본 공분산 $S$를 고유분해하여 최대우도해를 직접 구할 수 있습니다. 이 경우 $W$는 PCA의 주성분 방향과 일치하며, $\sigma^2$는 잔여 잡음 분산으로 추정됩니다.

2. **EM Algorithm**  
   숨겨진 변수 $z$를 도입하여 EM(Expectation–Maximization) 알고리즘으로 추정할 수 있습니다. 이는 결측치가 있는 경우나 대규모 데이터 처리에 유리합니다.

### Closed-form MLE (Derivation Sketch)
생성 모델 $x \sim \mathcal{N}(\mu, C),\ C = W W^\top + \sigma^2 I$에서 로그가능도는

$$
\mathcal{L}(W,\sigma^2) = -\tfrac{N}{2}\{\log|C| + \mathrm{tr}(C^{-1}S)\} + \text{const},\quad
S = \tfrac{1}{N}\sum_n (x_n-\bar x)(x_n-\bar x)^\top
$$

입니다. $S = U \Lambda U^\top$로 고유분해하고, $W = U A R$로 두면 목적함수는 축별로 분리됩니다. 계산을 통해

$$
a_i^2 = \lambda_i - \sigma^2 \quad (i \le q), \qquad
\hat\sigma^2 = \frac{1}{d-q}\sum_{i=q+1}^d \lambda_i
$$

가 유도됩니다. 따라서 최종 해는

$$
\hat W = U_q (\Lambda_q - \hat\sigma^2 I_q)^{1/2} R,\quad
\hat\mu = \bar x
$$

입니다. 여기서 $R$은 임의의 직교행렬로, 회전 불식별성을 나타냅니다.

### EM Algorithm (Update Equations)
EM에서는 숨겨진 변수 $z$의 사후분포를 활용합니다.

**E-step**  
$M = W^\top W + \sigma^2 I_q$라 하면,

$$
\mathbb{E}[z_n \mid x_n] = M^{-1} W^\top (x_n - \mu),\quad
\mathbb{E}[z_n z_n^\top \mid x_n] = \sigma^2 M^{-1} + \mathbb{E}[z_n]\mathbb{E}[z_n]^\top
$$

**M-step**  
집계량을 $S_{xz} = \tfrac{1}{N}\sum (x_n-\mu)\mathbb{E}[z_n]^\top,\ 
S_{zz} = \tfrac{1}{N}\sum \mathbb{E}[z_n z_n^\top]$라 하면,

$$
W \leftarrow S_{xz} S_{zz}^{-1},\quad \mu \leftarrow \tfrac{1}{N}\sum x_n
$$

$$
\sigma^2 \leftarrow \frac{1}{d}\Bigg[\frac{1}{N}\sum_n \|x_n-\mu\|^2 - 2\,\mathrm{tr}(W^\top S_{xz})+ \mathrm{tr}(W^\top W S_{zz})\Bigg]
$$

으로 업데이트합니다. 수렴 시 해는 Closed-form과 일치합니다.

## Relation to PCA
- PPCA에서 $\hat W$는 PCA의 주성분 벡터와 동일합니다.  
- 따라서 PPCA는 PCA의 확률적 일반화로 해석됩니다.  
- 차이점은 PPCA가 공분산에 등방성 잡음을 포함시켜 불확실성을 모델링한다는 점입니다.  
- 직관적으로, 큰 고유값 방향은 잠재 변수 $z$로 설명되고, 작은 고유값은 잡음 $\sigma^2$로 흡수됩니다.

## PCA–PPCA–FA–BPCA Relation
- **PCA**: 순수 선형대수적 차원축소  
- **PPCA**: PCA를 확률적 생성 모델로 재해석 (등방성 노이즈)  
- **FA (Factor Analysis)**: 노이즈 공분산을 대각 행렬 $\Psi$로 일반화 
- **BPCA (Bayesian PCA)**: $W,\sigma^2$에 사전 분포를 두어 차원 자동 선택, 과적합 방지  

즉, PCA ⊂ PPCA ⊂ FA, 그리고 PPCA → BPCA로 확장 가능합니다.

## Advantages
1. 결측치 처리 용이 (EM으로 결측치를 추론하며 학습 가능)  
2. 불확실성 추정 가능 (분포적 정의 덕분에 신뢰구간 제공 가능)  

## Conclusion
PPCA는 PCA를 확률적 생성 모델로 재해석한 기법입니다.  
PCA의 주성분 방향을 그대로 유지하면서도 결측치 처리, 불확실성 추정, Bayesian 확장 같은 추가적 장점을 제공합니다.  

## References
- Tipping, M. E., & Bishop, C. M. (1999). *Probabilistic Principal Component Analysis*. JRSS: Series B.  
- TensorFlow Probability: [Probabilistic PCA Example](https://www.tensorflow.org/probability/examples/Probabilistic_PCA?hl=ko)
