---
layout: single
title: "Probabilistic PCA"
date: 2025-09-30
permalink: /ppca/
categories:
  - Statistics
  - Bayesian
tags:
  - Dimensionality Reduction
  - PPCA
  - PCA

toc: true
toc_sticky: true
comments: true
---

> 이 포스팅은 [Probabilistic Principal Component Analysis (PPCA)](https://www.cs.columbia.edu/~blei/seminar/2020-representation/readings/TippingBishop1999.pdf)에 대해 공부하며 정리한 글입니다.  
> Tipping & Bishop (1999)의 고전 논문을 기반으로 수학적 정의, 추론 과정, 장단점을 설명하겠습니다.

## Background
- **PCA**는 데이터의 공분산 행렬을 고유분해하여 저차원 공간으로 투영하는 선형 차원축소 기법입니다.  
- 하지만 PCA는 **선형 대수적 접근**일 뿐, 확률 모델로 정의되지 않습니다.  
- 따라서
  - 불확실성을 추정 할 수 없고
  - 결측치(missing data) 처리가 불가능하며
  - 다른 확률적 모델과의 통합이 어렵다는 단점이 존재합니다. 
> 이를 해결하기 위해 PCA를 **확률적 생성 모델**로 재해석한 것이 PPCA입니다.


## TL;DR - Easy Analogy
- 데이터를 “풍선”이라고 생각해보면

- 풍선은 몇몇 축으로 크게 부풀고(=주성분), 나머지는 그냥 대충 동그란 모양(=노이즈).

- PCA는 **“풍선이 어디로 제일 늘어났는지”** 만 보는 거고,

- PPCA는 **“풍선이 늘어난 부분 + 바탕에 깔린 동그란 바람(노이즈)”** 까지 모델링하는 거라고 생각할 수 있습니다!


## Model Definition
PPCA는 잠재변수 모형(latent variable model)로 정의됩니다. 즉, 데이터는 숨겨진 저차원 변수로부터 생성된다고 가정합니다.

$$
x = W z + \mu + \epsilon
$$

- $x \in \mathbb{R}^d$: 관측 벡터  
- $z \in \mathbb{R}^q$: 잠재 벡터 ($q \ll d$)  
- $W \in \mathbb{R}^{d \times q}$: 로딩 행렬  
- $\mu \in \mathbb{R}^d$: 평균 벡터  
- $\epsilon \sim \mathcal{N}(0, \sigma^2 I_d)$: 등방성 가우시안 노이즈  
- $z \sim \mathcal{N}(0, I_q)$: 표준 정규 분포  

따라서 주변 분포는 다음과 같습니다.

$$
x \sim \mathcal{N}(\mu, W W^\top + \sigma^2 I_d)
$$

즉 PCA가 찾는 저차원 공간을 **확률적 생성 모델**로 정의한 것이 PPCA입니다.

## Parameter Estimation
목표는 $W, \mu, \sigma^2$를 학습하는 것입니다. 데이터 로그가능도를 최대화하여 추정합니다.

### Two Approaches
1. **Closed-form MLE**  
   표본 공분산 $S$를 고유분해하여 최대우도해를 직접 구할 수 있습니다. 이 경우 $W$는 PCA의 주성분 방향과 일치하며, $\sigma^2$는 잔여 잡음 분산으로 추정됩니다.

2. **EM Algorithm**  
   숨겨진 변수 $z$를 도입하여 EM(Expectation–Maximization) 알고리즘으로 추정할 수 있습니다. 이는 결측치가 있는 경우나 대규모 데이터 처리에 유리합니다.

### Closed-form MLE (Derivation Sketch)
생성 모델 $x \sim \mathcal{N}(\mu, C),\ C = W W^\top + \sigma^2 I$에서 로그가능도는

$$
\mathcal{L}(W,\sigma^2) = -\tfrac{N}{2}\{\log|C| + \mathrm{tr}(C^{-1}S)\} + \text{const},\quad
S = \tfrac{1}{N}\sum_n (x_n-\bar x)(x_n-\bar x)^\top
$$

입니다. $S = U \Lambda U^\top$로 고유분해하고, $W = U A R$로 두면 목적함수는 축별로 분리됩니다. 계산을 통해

$$
a_i^2 = \lambda_i - \sigma^2 \quad (i \le q), \qquad
\hat\sigma^2 = \frac{1}{d-q}\sum_{i=q+1}^d \lambda_i
$$

가 유도됩니다. 따라서 최종 해는

$$
\hat W = U_q (\Lambda_q - \hat\sigma^2 I_q)^{1/2} R,\quad
\hat\mu = \bar x
$$

입니다. 여기서 $R$은 임의의 직교행렬로, 회전 불식별성을 나타냅니다.

### EM Algorithm (Update Equations)
EM에서는 숨겨진 변수 $z$의 사후분포를 활용합니다.

**E-step**  
$M = W^\top W + \sigma^2 I_q$라 하면,

$$
\mathbb{E}[z_n \mid x_n] = M^{-1} W^\top (x_n - \mu),\quad
\mathbb{E}[z_n z_n^\top \mid x_n] = \sigma^2 M^{-1} + \mathbb{E}[z_n]\mathbb{E}[z_n]^\top
$$

**M-step**  
집계량을 $S_{xz} = \tfrac{1}{N}\sum (x_n-\mu)\mathbb{E}[z_n]^\top,\ 
S_{zz} = \tfrac{1}{N}\sum \mathbb{E}[z_n z_n^\top]$라 하면,

$$
W \leftarrow S_{xz} S_{zz}^{-1},\quad \mu \leftarrow \tfrac{1}{N}\sum x_n
$$

$$
\sigma^2 \leftarrow \frac{1}{d}\Bigg[\frac{1}{N}\sum_n \|x_n-\mu\|^2 - 2\,\mathrm{tr}(W^\top S_{xz})+ \mathrm{tr}(W^\top W S_{zz})\Bigg]
$$

으로 업데이트합니다. 수렴 시 해는 Closed-form과 일치합니다.

## Relation to PCA
- PPCA에서 $\hat W$는 PCA의 주성분 벡터와 동일합니다.  
- 따라서 PPCA는 PCA의 확률적 일반화로 해석됩니다.  
- 차이점은 PPCA가 공분산에 등방성 잡음을 포함시켜 불확실성을 모델링한다는 점입니다.  
- 직관적으로, 큰 고유값 방향은 잠재 변수 $z$로 설명되고, 작은 고유값은 잡음 $\sigma^2$로 흡수됩니다.


## Experiments

![3D PPCA](/assets/img/ppca/output.png/)
간단한 PPCA의 시각화입니다. 파란점들이 데이터이며, 두 개의 직선이 PPCA가 찾은 주성분 방향을 의미합니다. 중심의 구는 PPCA를 통해 추정한 등방성 노이즈 영역을 의미합니다. 

{% include code-header.html %}
```python
import numpy as np
import matplotlib.pyplot as plt

# ----- 1. 데이터 생성 (3D, 잠재 2D + 등방성 노이즈) -----
np.random.seed(0)
N = 500
d, q = 3, 2

# True 파라미터
mu_true = np.array([0.0, 0.0, 0.0])
W_true = np.array([[2.5, 0.0],
                   [0.0, 1.5],
                   [0.0, 0.0]])
sigma2_true = 0.5

# 잠재변수 z
Z = np.random.randn(q, N)
# 데이터 생성
X = (W_true @ Z + mu_true.reshape(-1,1) +
     np.sqrt(sigma2_true) * np.random.randn(d, N))

# ----- 2. PCA (고유분해) -----
S = np.cov(X)
eigvals, eigvecs = np.linalg.eigh(S)
order = np.argsort(eigvals)[::-1]
eigvals, eigvecs = eigvals[order], eigvecs[:, order]
Uq = eigvecs[:, :q]

# ----- 3. 구(노이즈) 좌표 -----
phi, theta = np.mgrid[0:np.pi:30j, 0:2*np.pi:30j]
r = np.sqrt(sigma2_true)
xs = r*np.sin(phi)*np.cos(theta) + mu_true[0]
ys = r*np.sin(phi)*np.sin(theta) + mu_true[1]
zs = r*np.cos(phi) + mu_true[2]

# ----- 4. Helper: equal axis ratio -----
def set_axes_equal(ax):
    """Set 3D plot axes to equal scale."""
    x_limits = ax.get_xlim3d()
    y_limits = ax.get_ylim3d()
    z_limits = ax.get_zlim3d()

    x_range = abs(x_limits[1] - x_limits[0])
    x_middle = np.mean(x_limits)
    y_range = abs(y_limits[1] - y_limits[0])
    y_middle = np.mean(y_limits)
    z_range = abs(z_limits[1] - z_limits[0])
    z_middle = np.mean(z_limits)

    plot_radius = 0.5*max([x_range, y_range, z_range])

    ax.set_xlim3d([x_middle - plot_radius, x_middle + plot_radius])
    ax.set_ylim3d([y_middle - plot_radius, y_middle + plot_radius])
    ax.set_zlim3d([z_middle - plot_radius, z_middle + plot_radius])

# ----- 5. 시각화 -----
fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(111, projection='3d')

# 데이터 (투명하게)
ax.scatter(X[0], X[1], X[2], alpha=0.05, color="blue")

# 평균
ax.scatter(*mu_true, color='red', s=100, label="mean")

# 주성분 축
for i in range(q):
    vec = Uq[:, i]
    line = np.array([mu_true - 4*vec, mu_true + 4*vec])
    ax.plot(line[:,0], line[:,1], line[:,2], lw=3, color="black", label=f"axis {i+1}")

# 노이즈 구
ax.plot_surface(xs, ys, zs, color='orange', alpha=0.3, linewidth=0)

set_axes_equal(ax)
ax.set_title("PPCA: principal axes + isotropic noise sphere")
ax.legend()
plt.show()

```


## Advantages
1. 결측치 처리 용이 (EM으로 결측치를 추론하며 학습 가능)  
2. 불확실성 추정 가능 (분포적 정의 덕분에 신뢰구간 제공 가능)  

## Conclusion
PPCA는 PCA를 확률적 생성 모델로 재해석한 기법입니다.  
PCA의 주성분 방향을 그대로 유지하면서도 결측치 처리, 불확실성 추정, Bayesian 확장 같은 추가적 장점을 제공합니다.  

## References
- [Tipping, M. E., & Bishop, C. M. (1999). *Probabilistic Principal Component Analysis*. JRSS: Series B.](https://www.cs.columbia.edu/~blei/seminar/2020-representation/readings/TippingBishop1999.pdf)
- TensorFlow Probability: [Probabilistic PCA Example](https://www.tensorflow.org/probability/examples/Probabilistic_PCA?hl=ko)
