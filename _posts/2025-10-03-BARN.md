---
layout: single
title: "Bayesian Additive Regression Networks (BARN)"
date: 2025-10-03
permalink: /barn/
categories:
  - Machine Learning
  - Bayesian
tags:
  - Bayesian Additive Regression Networks
  - Regression

toc: true
toc_sticky: true
comments: true
---

> 이 포스팅은 **Bayesian Additive Regression Networks (BARN)** [*Linero, 2024; arXiv:2404.04425v1*](https://arxiv.org/abs/2404.04425) 을 읽고 정리한 글입니다.

BART(Bayesian Additive Regression Trees)의 사상을 **신경망(Neural Network)** 으로 확장하여, 베이지안 모델의 불확실성 추론을 유지하면서 **부드럽고 연속적인 함수 근사**를 수행할 수 있습니다.

---

## Background: From BART to BARN

BART(Bayesian Additive Regression Trees)는 다음과 같이 데이터를 모델링합니다.

$$
y_i = \sum_{t=1}^{T} g(x_i; T_t, M_t) + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0,\sigma^2)
$$

여기서 각 $g(x;T_t,M_t)$는 작은 회귀트리(weak learner)이며,  
전체 예측은 이들의 **Additive Sum** 으로 표현됩니다.

BART는 불확실성 추정과 Regularization에 강하지만, 트리 구조의 특성상 각 트리가 구간별로 상수값을 예측하므로 전체 예측함수는 불연속적입니다.

즉, 회귀나 분류 모두 가능하지만,
예측 함수의 모양이 매끄럽지 않아 부드러운(smooth) 관계를 표현하는 데 한계가 있습니다.


---

## Motivation: Improve BART's Limitation

BARN은 **“트리 대신 작은 신경망을 더하자”** 는 아이디어로 출발합니다.

$$
f(x) = \sum_{k=1}^{K} M_k(x; \theta_k)
$$

- $M_k(x; \theta_k)$ : 작은 신경망 
- $\theta_k = \{W_k, b_k, \beta_k\}$ : 가중치 집합  
- 각 $M_k$는 **weak learner**로, 전체 모델은 이들의 합으로 구성됨  

즉, **BARN = BART의 Bayesian 구조 + Neural Network의 연속성(smoothness)**

---

## Model Structure

각 신경망은 다음 형태를 갖습니다.

$$
M_k(x; \theta_k) = \beta_k^\top \phi(W_k x + b_k)
$$

- $\phi(\cdot)$ : 활성화 함수 (ReLU, tanh 등)  
- $W_k, b_k, \beta_k$ : 학습할 파라미터  
- 출력은 scalar  

전체 예측식은 다음과 같습니다.

$$
y_i = \sum_{k=1}^{K} M_k(x_i; \theta_k) + \varepsilon_i, \quad
\varepsilon_i \sim \mathcal{N}(0,\sigma^2)
$$

---

## Bayesian Backfitting with Neural Networks

BARN은 BART와 동일하게 **Bayesian backfitting**을 수행합니다.
Bayesian Backfitting에 대한 글은 [BART](/bart/) 포스팅을 참고해주세요.

즉, 한 번에 하나의 신경망만 갱신하고, 나머지는 고정시킵니다.

$$
R_k = Y - \sum_{j\neq k} M_j(X; \theta_j)
$$

이때 $R_k$는 현재 $k$번째 네트워크가 설명해야 할 잔차이며, 
$M_k$는 $R_k$를 근사하도록 학습됩니다.

이 과정은 Additive Residual Fitting을 통해 Bias를 줄이면서도  
각 네트워크가 작은 역할만 하도록 제한해 오버피팅을 방지합니다.

---

## RJMCMC for Model Size

각 신경망의 **은닉 뉴런 수**를 RJMCMC(Reversible Jump MCMC)로 업데이트합니다.

- **BIRTH move:** 뉴런 1개 추가  
- **DEATH move:** 뉴런 1개 제거  

새로운 제안모델 $M_k'$은 다음 확률로 수락됩니다.

$$
A = \min\!\left(
1,
\frac{
T(M_k | M_k')\, P(R_k | X, M_k')\, P(M_k')
}{
T(M_k' | M_k)\, P(R_k | X, M_k)\, P(M_k)
}
\right)
$$

- $T(\cdot)$ : 제안 확률  
- $P(M_k)$ : 모델 사전분포 (예: 뉴런 수에 Poisson(λ=1) prior)  
- $P(R_k|X,M_k)$ : 잔차에 대한 우도 (다음 섹션에서 근사함)

---

## The Challenge: Weight Marginalization

이론적으로는 다음 적분을 계산해야 합니다.

$$
P(R_k|M_k)
= \int P(R_k|M_k, w_k)\, P(w_k)\, dw_k
$$

그러나 신경망의 가중치 수가 매우 많기 때문에, 이 적분은 닫힌 형태의 계산이 불가능합니다. 

---

## Peak Approximation (MAP Approximation)

Linero(2024)는 이 적분을 **“Peak Approximation”** 로 대체합니다.  

즉, 적분을 하지 말고, **우도의 봉우리(peak)**, 즉 가장 가능성이 높은 가중치 $w^*$에서의 값으로 근사하자는 것입니다.

$$
P(R_k|M_k)
\approx P(R_k|M_k, w_k^*),
\quad
w_k^* = \arg\max_{w} P(R_k|M_k,w)\,P(w)
$$

이는 가중치의 **MAP 추정(Maximum A Posteriori)** 에 해당합니다.

실제로는 잔차 $R_k$에 대해 신경망을 짧게 학습(BFGS 100회 정도)하여 $w_k^*$를 얻고,  
그때의 우도 값을 수락비에 사용합니다.

또한 오버피팅 방지를 위해 우도는 학습 데이터가 아닌 **검증셋(validation set)** 에 대해 계산합니다.

---

## Validation Likelihood Approximation

정규 오차를 가정하면 다음과 같습니다.

$$
P(R_k|X,M_k)
\approx
\prod_{j\in \text{valid}}
\frac{1}{\sigma\sqrt{2\pi}}
\exp\!\left(
-\frac{(R_{k,j} - M_k(X_j;w_k^*))^2}{2\sigma^2}
\right)
$$

이는 실제 적분을 “봉우리의 높이”로 대체한 pseudo-likelihood 형태로 볼 수 있습니다. 검증셋 기반이기 때문에 일반화 성능이 높은 모델이 더 자주 수락되게 됩니다.

---

## Experimental Results

BARN은 다양한 회귀 데이터셋에서  
BART 및 일반 신경망(CV 튜닝 버전)보다 낮은 RMSE를 기록하였습니다.

| Dataset | BARN | BART | Big NN (CV) |
|----------|------|------|-------------|
| Concrete | **0.375** | 0.533 | 0.702 |
| Airfoil  | **0.061** | 0.075 | 0.080 |
| Housing  | **0.052** | 0.084 | 0.077 |

비록 근사적 베이지안 모델이지만,  
잔차 기반 백피팅과 peak 근사 덕분에 일반화 성능이 우수함을 보였습니다.

---

## BARN vs BART

| 항목 | BART | BARN |
|------|------|------|
| 기본 단위 | 결정트리 | 얕은 신경망 |
| 모델 형태 | 계단형(piecewise constant) | 매끄러운(smooth) |
| 우도 계산 | analytic (공액성 이용) | peak 근사 (non-conjugate) |
| 학습 | Gibbs / Backfitting | RJMCMC + short BFGS |
| 장점 | 정확한 베이지안 추론 | 표현력, 일반화, 튜닝프리 |
| 단점 | 불연속적, 계단형 | 비엄밀(근사), 계산량 증가 |

---

## Why Peak Approximation Works: Flat Posterior in Neural Networks

BARN은 “봉우리 하나의 값(peak)”만으로 적분을 대체합니다.
그런데 신경망의 사후분포 $p(w|D)$가 실제로 **flat(평평)** 하기 때문에,  
이 근사가 실전에서는 잘 작동합니다.

---

### Neural Network Posterior is Multimodal but Flat

신경망의 사후분포는 일반적으로 다음과 같은 특징을 갖습니다.

- **Multimodal (다봉우리):** 여러 지역(local minima)이 존재하지만 대부분의 봉우리가 비슷한 손실(loss)을 가짐  
- **Flat Minima (평평한 최소점):** 약간의 가중치 변화에도 성능이 거의 변하지 않음  
- **Parameter Symmetry (대칭성):** 은닉 뉴런 순서를 바꿔도 동일한 함수가 만들어짐  

결과적으로 $p(w|D)$는 sharp하지 않고, **넓고 완만한 고원(flat basin)** 형태를 띱니다.

---

### Flat Minima와 일반화의 관계

[Hochreiter & Schmidhuber (1997)](https://direct.mit.edu/neco/article-abstract/9/1/1/6027/Flat-Minima?redirectedFrom=fulltext)은 flat minima 근처의 모델이 sharp minima보다 일반화 성능이 높음을 보였습니다. flat region에서는 $p(w|D)$가 완만하므로, “적분(mean)”과 “봉우리(peak)”가 거의 동일합니다.

$$
E[f(x;w)] \approx f(x; w^*)
$$

---

### Optimization as MAP Estimation

BARN의 각 신경망은 짧은 BFGS 학습을 수행하며, 이는 posterior의 봉우리 근처에서 $w^*$를 찾는 **MAP 추정 과정**에 해당합니다. 짧은 학습 덕분에 sharp minima로 가지 않고, flat region 내에서 멈춰 peak 근사로도 안정적인 예측이 가능합니다.

---

### Ensemble Averaging Effect

여러 작은 신경망을 additive하게 결합하면  
서로 다른 지역(local optima)의 $w^*$를 평균내는 효과가 생깁니다.

$$
f(x) = \sum_k M_k(x; w_k^*)
$$

따라서 BARN 전체는 posterior의 여러 봉우리 평균값을 근사하며,  
이는 실제 베이지안 모델 평균화(Bayesian Model Averaging)와 유사한 효과를 내게됩니다.

---

## Summary

- BARN은 BART의 Bayesian 구조를 유지하면서 트리 대신 신경망을 가법적으로 결합한 모델입니다.  
- 가중치 적분은 불가능하므로 peak(MAP) 근사로 대체합니다.
- 신경망의 flat posterior 덕분에 이 근사법이 실전에서 잘 작동합니다.
- Validation 기반 likelihood를 사용해 일반화 중심의 모델 선택이 가능합니다.
- 결과적으로 “비엄밀하지만 강력한 베이지안 근사” 모델로 볼 수 있습니다.

---

## References

- Linero, A. (2024). [*Bayesian Additive Regression Networks*](https://arxiv.org/abs/2404.04425). arXiv:2404.04425v1.  
- Hochreiter, S., & Schmidhuber, J. (1997). [*Flat Minima and Generalization in Neural Networks*](https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.1.1). *Neural Computation, 9*(1), 1–42.  
- Keskar, N. S., et al. (2016). [*On Large-Batch Training and Sharp Minima*](https://arxiv.org/abs/1609.04836). *ICLR*.  
- Izmailov, P., et al. (2018). [*Averaging Weights Leads to Wider Optima in Deep Learning*](https://arxiv.org/abs/1803.05407). *UAI*.  
- Wilson, A. G., & Izmailov, P. (2020). [*Bayesian Deep Learning and Deep Ensembles: A Tale of Two Approaches*](https://proceedings.neurips.cc/paper/2020/hash/7e7cecc16e1d7d92e34b423fc9e6a1c6-Abstract.html). *NeurIPS Tutorial*.