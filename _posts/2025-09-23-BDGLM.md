---
layout: single  
title: "[Paper Review] Bayesian Deep Net GLM and GLMM"  
date: 2025-09-23  
permalink: /BDGLM/  
categories:  
  - Statistics
  - Machine Learning
  - Bayesian
tags:  
  - Bayesian Deep Learning
  - Bayesian Deep Net GLM & GLMM
  - Generalized Linear Models

toc: true  
toc_sticky: true  
---

> 이 포스팅은 Polson, Sokolov, & Spiliopoulos (2018)의 논문  
> [Bayesian Deep Net GLM and GLMM](https://arxiv.org/pdf/1805.10157)을 읽고 정리한 글입니다.  

---

## Introduction

Generalized Linear Models (GLM)과 Generalized Linear Mixed Models (GLMM)은  
**통계적 해석력과 엄밀한 추론**을 제공하지만, 복잡한 데이터에서 **표현력의 한계**를 갖습니다.  

반대로 Deep Neural Networks(DNN)은 **강력한 표현력**을 가지지만,  
**불확실성 추정과 해석력 부족**이라는 단점이 있습니다.  

이 논문은 이 두 가지 세계를 결합하여,  
**Bayesian Deep Net GLM/GLMM**이라는 새로운 프레임워크를 제시합니다.  

---

## Background: GLM and GLMM

- **GLM**:  
  \[
  y_i \sim \text{ExpFam}(\eta_i), \quad \eta_i = x_i^\top \beta
  \]

  여기서 \(\eta_i\)는 선형 예측자(linear predictor),  
  지수족 분포(Exponential Family)와 링크 함수(link function)를 통해 연결됩니다.  

- **GLMM**:  
  \[
  \eta_i = x_i^\top \beta + z_i^\top u, \quad u \sim N(0, \Sigma)
  \]

  집단 효과(random effects)를 포함해 계층적 구조를 모델링합니다.  

---

## Bayesian Deep Net GLM

저자들은 선형 구조를 고정하지 않고,  
**Deep Neural Network를 통해 비선형 매핑을 학습**합니다:

\[
g(\mathbb{E}[y|x]) = f_\theta(x), \quad f_\theta \in \text{Deep NN}
\]

- **핵심 아이디어**: 링크 함수나 선형 예측자를 DNN으로 대체.  
- **Bayesian 처리**: \(\theta\)에 prior를 두고 full posterior 추정.  

---

## Polya-Gamma Data Augmentation

Logistic/Negative-Binomial GLM 추론에서 유명한 **Polya-Gamma(Polson et al., 2013)** 기법을 확장합니다.  

\[
\frac{(e^\psi)^a}{(1+e^\psi)^b} 
= 2^{-b} e^{(a - b/2)\psi} 
\int_0^\infty e^{-\omega \psi^2/2} p(\omega) d\omega, \quad \omega \sim PG(b,0)
\]

- 이 아이디어 덕분에,  
  딥넷 구조에서도 **conditionally Gaussian form**을 유지 → Gibbs Sampling 가능.  
- 결과적으로 Bayesian Deep Net GLM도 **MCMC 기반 추론**이 실현됩니다.  

---

## Extension to GLMM

- Random effects 구조를 포함하여 Bayesian Deep Net을 **계층적 모델**로 확장.  
- 예시: 개인별, 집단별 효과가 존재하는 데이터에서 **Bayesian Deep Net GLMM**으로 학습 가능.  

---

## Experiments

- **데이터셋**: MNIST, 영화 리뷰 감정 분류, mixed-effects 데이터 등  
- **결과**:
  - 전통적 GLM/GLMM 대비 **예측력 개선**  
  - 순수 DNN 대비 **불확실성 추정 및 해석 가능성 확보**  

---

## Discussion

- **장점**:
  - GLM의 해석력 + DNN의 표현력 결합  
  - Polya-Gamma augmentation으로 Bayesian inference 가능  
- **한계**:
  - Sampling 기반 → 계산 비용이 큼  
  - GLM 수준의 단순 해석력은 여전히 확보하기 어려움  
- **미래 확장**:
  - Stochastic Gradient MCMC, Variational Inference 등 scalable 추론 기법 적용 가능  

---

## Conclusion

이 논문은 **통계 모델링과 딥러닝의 경계**에서 중요한 시도를 보여줍니다.  
단순 GLM을 넘어서는 복잡한 데이터 구조를 다루면서도,  
베이지안 추론을 통해 **불확실성을 정량화**할 수 있는 길을 열어줍니다.  

---